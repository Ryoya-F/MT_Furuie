{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c1174-7a6d-49d4-a1df-5e337d0f452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5230b-91c8-48e1-bfd5-1e3ebd1fbcb8",
   "metadata": {},
   "source": [
    "# データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5bfad-192d-47e4-bf85-1eceab945f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ユーザ設定 ======\n",
    "\n",
    "excel_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\Trial_3\\Miwa_hourlyAve_for_LSTM_trial3.xlsx\"\n",
    "flood_idx_path_train = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\Trial_3\\Miwa_flood_idx_train.xlsx\"\n",
    "flood_idx_path_val = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\Trial_3\\Miwa_flood_idx_val.xlsx\"\n",
    "flood_idx_path_test = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\Trial_3\\Miwa_flood_idx_test.xlsx\"\n",
    "\n",
    "# 列番号（0始まり）で指定\n",
    "enc_cols   = [5, 3, 4]      # エンコーダ入力の列番号（例：3変数）\n",
    "dec_cols   = [5, 3]  # デコーダ入力の列番号（例：2変数）\n",
    "y_cols      = [4]          # 出力（目的変数）の列番号（例：1変数）\n",
    "\n",
    "Te = 72    # エンコーダのタイムステップ長\n",
    "Td = 240   # デコーダのタイムステップ長\n",
    "\n",
    "batch_size = 16 # ミニバッチサイズ\n",
    "\n",
    "# 洪水区間（1始まり行番号で指定してOK。Python内部で0始まりに直す）\n",
    "df_ranges_train = pd.read_excel(flood_idx_path_train, header=0)\n",
    "flood_ranges_train_1based = [tuple(x) for x in df_ranges_train.to_numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a7c8d-4770-4d55-8e96-8ec0b8c52632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 読み込み ======\n",
    "df = pd.read_excel(excel_path, header=0)\n",
    "\n",
    "# 0-basedに変換（pandasは0-based）\n",
    "flood_ranges_train = [(s-1, e-1) for (s, e) in flood_ranges_train_1based]\n",
    "\n",
    "# 必要列だけ抽出（順番固定）\n",
    "use_cols = enc_cols + dec_cols + y_cols\n",
    "data = df.iloc[:, use_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c7818-8c54-4ef6-8873-38a2475974e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 標準化：train dataから平均・標準偏差を算出 ======\n",
    "\n",
    "flood_data_train_parts = [data.iloc[s:e+1, :] for (s, e) in flood_ranges_train]\n",
    "flood_data_train = pd.concat(flood_data_train_parts, axis=0)\n",
    "\n",
    "mean = flood_data_train.mean(numeric_only=True)\n",
    "std  = flood_data_train.std(numeric_only=True).replace(0, 1.0)\n",
    "data_norm = (data - mean) / std # 全期間のデータを標準化\n",
    "\n",
    "print('---平均（train）----')\n",
    "print(mean)\n",
    "\n",
    "print('----標準偏差（train）----')\n",
    "print(std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3226e-f265-4ddc-abbb-93d8cc539d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 洪水区間ごとにスライド窓でサンプル作成（train） ======\n",
    "samples_train = []  # list of (enc_X: [Te, Fe], dec_X: [Td, Fd], y: [Td, Fo])\n",
    "Fe, Fd, Fo = len(enc_cols), len(dec_cols), len(y_cols)\n",
    "\n",
    "for (s, e) in flood_ranges_train:\n",
    "    seg = data_norm.iloc[s:e+1]  # 区間データ（両端含む）\n",
    "    n = len(seg)\n",
    "    if n < Te + Td: # 1サンプルは Te + Tdの長さが必要\n",
    "        continue\n",
    "        \n",
    "    for start in range(0, n - (Te + Td) + 1):\n",
    "        enc_window = seg.iloc[start : start + Te]\n",
    "        dec_window = seg.iloc[start + Te : start + Te + Td]\n",
    "        # テンソル化\n",
    "        enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))     # [Te, Fe]\n",
    "        dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))     # [Td, Fd]\n",
    "        y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))      # [Td, Fo]\n",
    "        samples_train.append((enc_X, dec_X, y))\n",
    "\n",
    "print(f\"作成サンプル数: {len(samples_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01144d4b-68d0-4692-813d-69cb60c50bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 洪水区間ごとにスライド窓でサンプル作成（val） ======\n",
    "\n",
    "df_ranges_val = pd.read_excel(flood_idx_path_val, header=0)\n",
    "flood_ranges_val_1based = [tuple(x) for x in df_ranges_val.to_numpy()]\n",
    "flood_ranges_val = [(s-1, e-1) for (s, e) in flood_ranges_val_1based]\n",
    "\n",
    "\n",
    "samples_val = []  # list of (enc_X: [Te, Fe], dec_X: [Td, Fd], y: [Td, Fo])\n",
    "Fe, Fd, Fo = len(enc_cols), len(dec_cols), len(y_cols)\n",
    "\n",
    "for (s, e) in flood_ranges_val:\n",
    "    seg = data_norm.iloc[s:e+1]  # 区間データ（両端含む）\n",
    "    n = len(seg)\n",
    "    if n < Te + Td: # 1サンプルは Te + Tdの長さが必要\n",
    "        continue\n",
    "        \n",
    "    for start in range(0, n - (Te + Td) + 1):\n",
    "        enc_window = seg.iloc[start : start + Te]\n",
    "        dec_window = seg.iloc[start + Te : start + Te + Td]\n",
    "        # テンソル化\n",
    "        enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))     # [Te, Fe]\n",
    "        dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))     # [Td, Fd]\n",
    "        y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))      # [Td, Fo]\n",
    "        samples_val.append((enc_X, dec_X, y))\n",
    "\n",
    "print(f\"作成サンプル数: {len(samples_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1677019-e74b-4fa3-99a0-61d270ef32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 洪水区間ごとにスライド窓でサンプル作成（test） ======\n",
    "\n",
    "df_ranges_test = pd.read_excel(flood_idx_path_test, header=0)\n",
    "flood_ranges_test_1based = [tuple(x) for x in df_ranges_test.to_numpy()]\n",
    "flood_ranges_test = [(s-1, e-1) for (s, e) in flood_ranges_test_1based]\n",
    "\n",
    "\n",
    "samples_test = []  # list of (enc_X: [Te, Fe], dec_X: [Td, Fd], y: [Td, Fo])\n",
    "Fe, Fd, Fo = len(enc_cols), len(dec_cols), len(y_cols)\n",
    "\n",
    "for (s, e) in flood_ranges_test:\n",
    "    seg = data_norm.iloc[s:e+1]  # 区間データ（両端含む）\n",
    "    n = len(seg)\n",
    "    if n < Te + Td: # 1サンプルは Te + Tdの長さが必要\n",
    "        continue\n",
    "        \n",
    "    for start in range(0, n - (Te + Td) + 1):\n",
    "        enc_window = seg.iloc[start : start + Te]\n",
    "        dec_window = seg.iloc[start + Te : start + Te + Td]\n",
    "        # テンソル化\n",
    "        enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))     # [Te, Fe]\n",
    "        dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))     # [Td, Fd]\n",
    "        y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))      # [Td, Fo]\n",
    "        samples_test.append((enc_X, dec_X, y))\n",
    "\n",
    "print(f\"作成サンプル数: {len(samples_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50ea38-9cfe-4875-a77b-a941b99a7958",
   "metadata": {},
   "source": [
    "# pyTorch Dataset / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e891d-b527-4c1d-8f98-08c91ffd8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, triplets):\n",
    "        self.triplets = triplets  # list of (enc_X, dec_X, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triplets[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc_seqs, dec_seqs, ys = zip(*batch)\n",
    "    # ここでは全サンプル同一長さ前提（Te/Td固定）なので単純stack\n",
    "    enc_x = torch.stack(enc_seqs, dim=0)  # [B, Te, Fe]\n",
    "    dec_x = torch.stack(dec_seqs, dim=0)  # [B, Td, Fd]\n",
    "    y     = torch.stack(ys,       dim=0)  # [B, Td, Fo]\n",
    "    \n",
    "    # マスク（将来可変長のときに使う）\n",
    "    B, Td, _ = y.shape\n",
    "    mask = torch.ones(B, Td, 1, dtype=torch.float32)\n",
    "    return enc_x, dec_x, y, mask\n",
    "\n",
    "\n",
    "trainDataset = FloodSeq2SeqDataset(samples_train)\n",
    "trainLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valDataset = FloodSeq2SeqDataset(samples_val)\n",
    "valLoader = DataLoader(valDataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "testDataset = FloodSeq2SeqDataset(samples_test)\n",
    "testLoader = DataLoader(testDataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55755df-49f9-44b5-8a2e-9ffb4ed74e98",
   "metadata": {},
   "source": [
    "# 損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f4213-343a-4271-a507-6dbf7db8d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(batch):\n",
    "    \"\"\"collate_fnの戻り値が (enc,dec,y) か (enc,dec,y,mask) のどちらでも対応\"\"\"\n",
    "    if len(batch) == 3:\n",
    "        enc_x, dec_x, y = batch\n",
    "        mask = torch.ones_like(y[..., :1])  # [B,Td,1]\n",
    "    elif len(batch) == 4:\n",
    "        enc_x, dec_x, y, mask = batch\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected batch format\")\n",
    "    return enc_x.to(device), dec_x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "def masked_mse(pred, target, mask):\n",
    "    # pred/target: [B,T,Out], mask: [B,T,1] (1=valid, 0=pad)\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask\n",
    "    denom = mask.sum(dim=(1,2)).clamp_min(1.0)  # per-sample\n",
    "    per_sample = diff2.sum(dim=(1,2)) / denom\n",
    "    return per_sample.mean()\n",
    "\n",
    "def masked_rmse(pred, target, mask):\n",
    "    return torch.sqrt(masked_mse(pred, target, mask))\n",
    "\n",
    "def masked_r2(pred, target, mask):\n",
    "    # R² = 1 - SSE/SST, マスク版\n",
    "    mean = (target * mask).sum(dim=(1,2), keepdim=True) / mask.sum(dim=(1,2), keepdim=True).clamp_min(1.0)\n",
    "    sse = ((pred - target) ** 2 * mask).sum(dim=(1,2))\n",
    "    sst = ((target - mean) ** 2 * mask).sum(dim=(1,2)).clamp_min(1e-12)\n",
    "    r2  = 1.0 - sse / sst\n",
    "    return r2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056610ed-099b-4966-bdaa-1f00b571489d",
   "metadata": {},
   "source": [
    "# 汎用Seq2Seq LSTM（エンコーダ→デコーダ、損失はマスクで集計）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5b144-c2c3-4f39-8d5d-e61435c5fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateBridge(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoderの(h, c)をDecoder初期状態へ写像するブリッジ。\n",
    "    - 層数/隠れ次元が異なってもOK\n",
    "    - bridge_mode:\n",
    "        - \"zero_pad\":  層合わせ=0埋め or 切り落とし、隠れ次元は線形射影\n",
    "        - \"repeat_top\":層合わせ=最上層の繰り返し/切り落とし、隠れ次元は線形射影\n",
    "        - \"linear_stack\": [B, L_enc, H_enc] -> 線形で [B, L_dec, H_dec] へ（層方向も学習で混合）\n",
    "\n",
    "    - enc_layers: エンコーダの層の深さ\n",
    "    - dec_layers: デコーダの層の深さ\n",
    "    - enc_hidden: エンコーダのノード数\n",
    "    - dec_hidden: デコーダのノード数\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_layers, dec_layers, enc_hidden, dec_hidden, mode=\"zero_pad\"):\n",
    "        super().__init__()\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.enc_hidden = enc_hidden\n",
    "        self.dec_hidden = dec_hidden\n",
    "        self.mode = mode\n",
    "\n",
    "        # 隠れ次元の変換（h/c共用）\n",
    "        if mode in (\"zero_pad\", \"repeat_top\"):\n",
    "            self.proj = nn.Linear(enc_hidden, dec_hidden, bias=True)\n",
    "        elif mode == \"linear_stack\":\n",
    "            # 層方向もまとめて線形変換\n",
    "            self.proj_h = nn.Linear(enc_layers * enc_hidden, dec_layers * dec_hidden, bias=True)\n",
    "            self.proj_c = nn.Linear(enc_layers * enc_hidden, dec_layers * dec_hidden, bias=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown bridge mode: {mode}\")\n",
    "\n",
    "    def _match_layers(self, x, how=\"zero_pad\"):\n",
    "        \"\"\"\n",
    "        x: [L_enc, B, H_enc] を層数だけ合わせる（隠れ次元は未変換）\n",
    "        return: [L_dec, B, H_enc]\n",
    "\n",
    "        B: バッチサイズ\n",
    "        \"\"\"\n",
    "        L_enc, B, H = x.shape\n",
    "        L_dec = self.dec_layers\n",
    "\n",
    "        if L_dec == L_enc:\n",
    "            return x\n",
    "\n",
    "        if L_dec < L_enc:\n",
    "            # 上位層を優先して切り落とす（直観的には最上層が一番抽象的）\n",
    "            return x[:L_dec, :, :]\n",
    "\n",
    "        # L_dec > L_enc の場合\n",
    "        pad_count = L_dec - L_enc\n",
    "        if how == \"repeat_top\":\n",
    "            top = x[-1:, :, :].expand(pad_count, B, H)  # 最上層を複製\n",
    "            return torch.cat([x, top], dim=0)\n",
    "        else:  # zero_pad\n",
    "            pad = x.new_zeros(pad_count, B, H)\n",
    "            return torch.cat([x, pad], dim=0)\n",
    "\n",
    "    def forward(self, h_enc, c_enc):\n",
    "        \"\"\"\n",
    "        h_enc, c_enc: [L_enc, B, H_enc]\n",
    "        返り値: (h0_dec, c0_dec) それぞれ [L_dec, B, H_dec]\n",
    "        \"\"\"\n",
    "        if self.mode in (\"zero_pad\", \"repeat_top\"):\n",
    "            # 層合わせ（まだ enc_hidden 次元のまま）\n",
    "            h = self._match_layers(h_enc, \"repeat_top\" if self.mode==\"repeat_top\" else \"zero_pad\")\n",
    "            c = self._match_layers(c_enc, \"repeat_top\" if self.mode==\"repeat_top\" else \"zero_pad\")\n",
    "            # 次元射影\n",
    "            L, B, H = h.shape\n",
    "            h = self.proj(h)  # broadcasting: [L,B,H_enc]->[L,B,H_dec]\n",
    "            c = self.proj(c)\n",
    "            return h, c\n",
    "\n",
    "        else:  # linear_stack\n",
    "            # [L_enc,B,H_enc] -> [B, L_enc*H_enc]\n",
    "            L_enc, B, H_enc = h_enc.shape\n",
    "            flat_h = h_enc.transpose(0,1).reshape(B, L_enc*H_enc)\n",
    "            flat_c = c_enc.transpose(0,1).reshape(B, L_enc*H_enc)\n",
    "            # 線形写像\n",
    "            out_h = self.proj_h(flat_h)  # [B, L_dec*H_dec]\n",
    "            out_c = self.proj_c(flat_c)\n",
    "            # [L_dec,B,H_dec] に戻す\n",
    "            L_dec, H_dec = self.dec_layers, self.dec_hidden\n",
    "            h = out_h.view(B, L_dec, H_dec).transpose(0,1).contiguous()\n",
    "            c = out_c.view(B, L_dec, H_dec).transpose(0,1).contiguous()\n",
    "            return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf2a71-0425-4702-96da-5b6232cfa057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_enc: int, # エンコーダ入力変数の数\n",
    "        in_dec: int, # デコーダ入力変数の数\n",
    "        out_dim: int, # 出力変数の数\n",
    "        enc_hidden: int = 128,\n",
    "        dec_hidden: int = 128,\n",
    "        enc_layers: int = 2,\n",
    "        dec_layers: int = 3,\n",
    "        bridge_mode: str = \"zero_pad\",  # \"zero_pad\" | \"repeat_top\" | \"linear_stack\"\n",
    "        dropout: float = 0.0, # LSTMの層間ドロップアウト率\n",
    "        bidirectional_enc: bool = False,  # エンコーダのみ双方向にするかどうか（必要ならEncoderを双方向にも）\n",
    "        head_activation=\"relu\", # 活性化関数\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional_enc = bidirectional_enc\n",
    "        enc_dir = 2 if bidirectional_enc else 1\n",
    "        enc_hidden_eff = enc_hidden * enc_dir  # 双方向なら出力次元が倍\n",
    "\n",
    "        self.enc = nn.LSTM(\n",
    "            input_size=in_enc,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=enc_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if enc_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional_enc\n",
    "        )\n",
    "\n",
    "        self.dec = nn.LSTM(\n",
    "            input_size=in_dec,\n",
    "            hidden_size=dec_hidden,\n",
    "            num_layers=dec_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dec_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Encoderが双方向のときは、(h_fwd, h_bwd) を結合した次元 enc_hidden_eff を\n",
    "        # Decoder hidden 次元へ写像する必要がある\n",
    "        self.bridge = StateBridge(\n",
    "            enc_layers=enc_layers * enc_dir,\n",
    "            dec_layers=dec_layers,\n",
    "            enc_hidden=enc_hidden,\n",
    "            dec_hidden=dec_hidden,\n",
    "            mode=bridge_mode\n",
    "        ) if (enc_layers != dec_layers or enc_dir != 1 or enc_hidden != dec_hidden or bridge_mode==\"linear_stack\") else None\n",
    "\n",
    "        self.head = nn.Linear(dec_hidden, out_dim) # 全結合層\n",
    "\n",
    "        self.act = {\n",
    "            \"identity\": nn.Identity(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "        }[head_activation]\n",
    "\n",
    "    def _extract_final_states(self, out, hc):\n",
    "        \"\"\"\n",
    "        LSTMの出力から (h_T, c_T) を取り出して成形。\n",
    "        双方向Encoderの場合は各層ごとに [fwd, bwd] を層方向に並べる。\n",
    "        \"\"\"\n",
    "        h, c = hc  # [num_layers * num_directions, B, H]\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        \"\"\"\n",
    "        enc_x: [B, Te, in_enc]\n",
    "        dec_x: [B, Td, in_dec]\n",
    "        return: yhat [B, Td, out_dim]\n",
    "        \"\"\"\n",
    "        _, (h_T, c_T) = self.enc(enc_x)  # h_T,c_T: [L_enc * dir, B, H_enc]\n",
    "\n",
    "        if self.bridge is not None:\n",
    "            h0_dec, c0_dec = self.bridge(h_T, c_T)  # [L_dec, B, H_dec]\n",
    "        else:\n",
    "            # 形・次元が完全一致ならそのまま\n",
    "            h0_dec, c0_dec = h_T, c_T\n",
    "\n",
    "        dec_out, _ = self.dec(dec_x, (h0_dec, c0_dec))  # [B, Td, H_dec]\n",
    "        yhat = self.act(self.head(dec_out))             # [B, Td, out_dim]\n",
    "        return yhat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfbd76-7603-4f15-bd85-82645d0acd76",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb4838-b64d-46b4-83bf-eff07d444af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = {\n",
    "    # Model\n",
    "    \"in_enc\": 3,\n",
    "    \"in_dec\": 2,\n",
    "    \"out_dim\": 1,\n",
    "    \"enc_hidden\": 128,\n",
    "    \"dec_hidden\": 128,\n",
    "    \"enc_layers\": 2,\n",
    "    \"dec_layers\": 3,\n",
    "    \"bridge_mode\": \"zero_pad\",   # \"zero_pad\" | \"repeat_top\" | \"linear_stack\"\n",
    "    \"dropout\": 0.1,\n",
    "    \"bidirectional_enc\": False,\n",
    "    \"head_activation\": \"relu\", # \"identity\", \"relu\", \"tanh\", \"sigmoid\" など\n",
    "    # Train\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 0.0, # L2正規化係数。0なら無効\n",
    "    \"grad_clip\": 1.0, # 勾配クリッピングの閾値。０かNoneなら無効\n",
    "    \"print_every\": 1, # 学習の進捗を何エポックごとに出力するか\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75568c-0825-4290-bf08-2b81b03840bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習関数・評価関数の定義\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        enc_x, dec_x, y, mask = split_batch(batch)\n",
    "        yhat = model(enc_x, dec_x)\n",
    "        loss = masked_mse(yhat, y, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if cfg[\"grad_clip\"]:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = enc_x.size(0) # バッチサイズ\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "    return total_loss / max(n,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0; total_rmse = 0.0; total_r2 = 0.0; n = 0\n",
    "    for batch in loader:\n",
    "        enc_x, dec_x, y, mask = split_batch(batch)\n",
    "        yhat = model(enc_x, dec_x)\n",
    "\n",
    "        loss = masked_mse(yhat, y, mask)\n",
    "        rmse = masked_rmse(yhat, y, mask)\n",
    "        r2   = masked_r2(yhat, y, mask)\n",
    "\n",
    "        bs = enc_x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_rmse += rmse.item() * bs\n",
    "        total_r2   += r2.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(n,1),\n",
    "        \"rmse\": total_rmse / max(n,1),\n",
    "        \"r2\":   total_r2   / max(n,1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1155d-44ae-4821-be2e-124b8caa544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル作成・最適化手法の決定・学習実行・保存\n",
    "\n",
    "# 例: train_loader, val_loader が既にある想定\n",
    "model = Seq2SeqLSTM(\n",
    "    in_enc=cfg[\"in_enc\"], in_dec=cfg[\"in_dec\"], out_dim=cfg[\"out_dim\"],\n",
    "    enc_hidden=cfg[\"enc_hidden\"], dec_hidden=cfg[\"dec_hidden\"],\n",
    "    enc_layers=cfg[\"enc_layers\"], dec_layers=cfg[\"dec_layers\"],\n",
    "    bridge_mode=cfg[\"bridge_mode\"], dropout=cfg[\"dropout\"],\n",
    "    bidirectional_enc=cfg[\"bidirectional_enc\"],\n",
    "    head_activation=cfg[\"head_activation\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    tr_loss = train_one_epoch(model, trainLoader, optimizer)\n",
    "    val_metrics = evaluate(model, valLoader)\n",
    "\n",
    "    if epoch % cfg[\"print_every\"] == 0:\n",
    "        print(f\"[{epoch}/{cfg['epochs']}] \"\n",
    "              f\"train_loss={tr_loss:.4f} | \"\n",
    "              f\"val_loss={val_metrics['loss']:.4f} \"\n",
    "              f\"val_rmse={val_metrics['rmse']:.4f} \"\n",
    "              f\"val_r2={val_metrics['r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f37f3-78f5-4574-9119-8365b17fc873",
   "metadata": {},
   "source": [
    "# テスト・モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60745e49-ab66-4d50-94ae-2a6c7bb6753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inverse_standardize_y_by_index(y_std, mean, std, data_columns, y_pos):\n",
    "    \"\"\"\n",
    "    y_std:  標準化スケールの出力テンソル [B, T, Fo]\n",
    "    mean, std: pandas.Series（学習時にfitしたもの。index=列名）\n",
    "    data_columns: 学習時に標準化した DataFrame の列名（例: data.columns）\n",
    "    y_pos: 出力列の「列番号（位置）」リスト（例: [Fe+Fd, Fe+Fd+1, ...]）\n",
    "\n",
    "    return: 元スケールの y [B, T, Fo]\n",
    "    \"\"\"\n",
    "    # 列番号 → 列名にマップ\n",
    "    y_colnames = [data_columns[i] for i in y_pos]\n",
    "\n",
    "    # 列名で mean/std を取り出してテンソル化（形を [1,1,Fo] に揃える）\n",
    "    m = torch.tensor([float(mean[c]) for c in y_colnames], device=y_std.device).view(1, 1, -1)\n",
    "    s = torch.tensor([float(std[c])  for c in y_colnames], device=y_std.device).view(1, 1, -1)\n",
    "\n",
    "    return y_std * s + m\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_original_scale_by_index(model, loader, mean, std, data_columns, y_pos):\n",
    "    model.eval()\n",
    "    tot_mse = tot_rmse = tot_r2 = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        if len(batch) == 3:\n",
    "            enc_x, dec_x, y_std = batch\n",
    "            mask = torch.ones_like(y_std[...,:1])\n",
    "        else:\n",
    "            enc_x, dec_x, y_std, mask = batch\n",
    "\n",
    "        enc_x = enc_x.to(mean.values.dtype if hasattr(enc_x, \"to\") else \"cpu\")\n",
    "        device = next(model.parameters()).device\n",
    "        enc_x, dec_x, y_std, mask = enc_x.to(device), dec_x.to(device), y_std.to(device), mask.to(device)\n",
    "\n",
    "        yhat_std = model(enc_x, dec_x)  # 標準化スケール\n",
    "\n",
    "        # --- 出力だけ逆標準化（列番号で指定） ---\n",
    "        yhat = inverse_standardize_y_by_index(yhat_std, mean, std, data_columns, y_pos)\n",
    "        y    = inverse_standardize_y_by_index(y_std,     mean, std, data_columns, y_pos)\n",
    "\n",
    "        # 指標（マスク付き）\n",
    "        mse  = masked_mse(yhat, y, mask).item()\n",
    "        rmse = mse ** 0.5\n",
    "        r2   = masked_r2(yhat, y, mask).item()\n",
    "\n",
    "        bs = enc_x.size(0)\n",
    "        tot_mse  += mse  * bs\n",
    "        tot_rmse += rmse * bs\n",
    "        tot_r2   += r2   * bs\n",
    "        n += bs\n",
    "\n",
    "    return {\"mse\": tot_mse/max(1,n), \"rmse\": tot_rmse/max(1,n), \"r2\": tot_r2/max(1,n)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff67ec8-34ff-472b-93c9-462f5de34b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = [Fe+Fd] # dataのうち、出力変数の列番号\n",
    "\n",
    "evaluate_original_scale_by_index(model, testLoader, mean, std, data.columns, y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f63051-1ba2-4f24-abbd-838558f226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\Tiral_3\\LSTM_trial_3\" # 保存するファイル名の指定\n",
    "\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"cfg\": cfg,\n",
    "    \"epoch\": epoch,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"scaler\": {\n",
    "        \"mean\": mean,                  # pandas.Series（index=列名）\n",
    "        \"std\":  std,                   # pandas.Series（index=列名）\n",
    "        \"data_columns\": list(data.columns),  # 学習時の列名順を保存\n",
    "        \"y_pos\": y_pos,                # 出力列の列番号（位置）\n",
    "    }\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7a309-cf0e-4dbc-b92a-b336d6aadc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c30486-03b7-4dad-b320-72212ee6cb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
