{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89b9875-e79b-4b07-bfb9-b175fcaaf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得られたモデルを用いて、濁度の予測を行う\n",
    "# １時間先の濁度予測だけでなく、n時間先の予測までを、行列として出力（縦：予測開始時刻、横：予測リードタイム）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce131b9-1b64-4875-979d-ff6dc0b23941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50647fa8-dbfe-416c-a986-741b9d7711d5",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1d5848-fd0c-4ca5-83e9-7404d0ac9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込むモデルの保存先のパスを指定【要変更】\n",
    "model_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_FFNN\\Trial_1\\best_model_trial_1.pth\"\n",
    "\n",
    "\n",
    "# データのパス指定【要確認】\n",
    "data_folder = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_FFNN_Data\\Trial_1\"\n",
    "# data_folder = r\"C:\\Users\\RYOYA\\OneDrive\\ドキュメント\\データ整理\\美和（修論）\\Miwa_FFNN\\Trial_1\"\n",
    "data_file_name = r\"Miwa_data_for_FFNN.xlsx\"\n",
    "idx_file_name = r\"Miwa_flood_idx_for_FFNN.xlsx\"\n",
    "data_path = os.path.join(data_folder, data_file_name)\n",
    "idx_path = os.path.join(data_folder, idx_file_name)\n",
    "\n",
    "\n",
    "# 予測結果保存先のエクセルファイル名【要変更】\n",
    "result_folder = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_FFNN\\Trial_1\"\n",
    "train_result_excel_name = r\"Miwa_FFNN_Trial_1_prediction_train.xlsx\"\n",
    "test_result_excel_name = r\"Miwa_FFNN_Trial_1_prediction_test.xlsx\"\n",
    "train_result_path = os.path.join(result_folder, train_result_excel_name)\n",
    "test_result_path = os.path.join(result_folder, test_result_excel_name)\n",
    "\n",
    "# input, output変数の列番号を指定（0始まり）\n",
    "# タイムラグもここで指定\n",
    "input_cols = [2, 2, 1, 1] # [Q(t), Q(t-1), Tur(t-1), Tur(t-2)]\n",
    "input_lags = [0, 1, 1, 2]\n",
    "output_cols = [1] # [Tur(t)]\n",
    "output_lags = [0]\n",
    "\n",
    "# ファイルの読み込み\n",
    "d_all = pd.read_excel(data_path, header=0)\n",
    "idx_list = pd.read_excel(idx_path, header=0)\n",
    "col_trial = 10 # 【要変更】どの列がtrain, testを指定する列か\n",
    "\n",
    "train_idx = idx_list[idx_list.iloc[:, col_trial] == 'train']\n",
    "test_idx = idx_list[idx_list.iloc[:, col_trial] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac7599-adfd-4cf3-a797-a837fb42c521",
   "metadata": {},
   "source": [
    "## モデル構造の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4393f3d-0e28-44c9-a0b7-5a847887e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "def build_ffnn(cfg: dict) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Feed Forward Neural Network (FFNN) を構築する関数。\n",
    "    cfg（辞書）でネットワーク構造・活性化関数・ドロップアウト率などを指定。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : dict\n",
    "        モデル設定を含む辞書。例：\n",
    "        {\n",
    "            \"input_dim\": 10,\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        指定条件に基づくFFNNモデル\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 活性化関数マッピング ---\n",
    "    activation_funcs = {\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"LeakyReLU\": nn.LeakyReLU(),\n",
    "        \"ELU\": nn.ELU(),\n",
    "        \"Sigmoid\": nn.Sigmoid(),\n",
    "        \"Tanh\": nn.Tanh(),\n",
    "        \"GELU\": nn.GELU()\n",
    "    }\n",
    "\n",
    "    act = activation_funcs.get(cfg.get(\"activation\", \"ReLU\"), nn.ReLU())\n",
    "    dropout_rate = cfg.get(\"dropout\", 0.0)\n",
    "    hidden_dims = cfg.get(\"hidden_dims\", [])\n",
    "    input_dim = cfg[\"input_dim\"]\n",
    "    output_dim = cfg[\"output_dim\"]\n",
    "\n",
    "    layers = []\n",
    "    in_dim = input_dim\n",
    "\n",
    "    # --- 隠れ層を順に構築 ---\n",
    "    for h in hidden_dims:\n",
    "        layers.append(nn.Linear(in_dim, h))\n",
    "        layers.append(act)\n",
    "        if dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        in_dim = h\n",
    "\n",
    "    # --- 出力層（活性化関数なし） ---\n",
    "    layers.append(nn.Linear(in_dim, output_dim))\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a95b0f-c259-4b87-9996-117554aa763f",
   "metadata": {},
   "source": [
    "## モデルの復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badf8190-574e-471d-ab18-1d4a6b7cfdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryoya\\AppData\\Local\\Temp\\ipykernel_7268\\3780748484.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# ハイパーパラメータ（cfg）を再現\n",
    "cfg_loaded = checkpoint[\"cfg\"]\n",
    "\n",
    "# モデル構築（cfgに基づいて同じ構造を作る）\n",
    "model = build_ffnn(cfg_loaded)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 標準化パラメータなども復元\n",
    "x_mean = checkpoint[\"x_mean\"]\n",
    "x_std  = checkpoint[\"x_std\"]\n",
    "y_mean = checkpoint[\"y_mean\"]\n",
    "y_std  = checkpoint[\"y_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2aeb8-6dee-440a-8902-b15ba9159e26",
   "metadata": {},
   "source": [
    "## 洪水イベントごとに予測を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a2787e-ace4-4110-bb2e-8c6104b9ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定範囲の抽出関数\n",
    "def slice_L_nan(df, col, start, end, L):\n",
    "    \"\"\"\n",
    "    df（例: d_all）から指定範囲を切り出し、\n",
    "    データが存在しない部分は NaN でパディングして (L,1) 配列として返す。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        元データ\n",
    "    col : int または str\n",
    "        取り出す列（列番号または列名）\n",
    "    start, end : int\n",
    "        切り出す範囲（両端含む）\n",
    "    L : int\n",
    "        期待する行数（end - start + 1 など）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr : numpy.ndarray, shape=(L,1)\n",
    "        NaN パディング済みの配列\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "\n",
    "    # 範囲をクリップ\n",
    "    start_clip = max(0, start)\n",
    "    end_clip   = min(n - 1, end)\n",
    "\n",
    "    # 実データを取得\n",
    "    arr = df.iloc[start_clip:end_clip+1, col].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # パディング数を計算\n",
    "    pad_top = max(0, 0 - start)             # start < 0 → 上をNaNで埋める\n",
    "    pad_bottom = max(0, end - (n - 1))      # end > n-1 → 下をNaNで埋める\n",
    "\n",
    "    # NaN パディングを適用\n",
    "    if pad_top > 0 or pad_bottom > 0:\n",
    "        arr = np.pad(arr, ((pad_top, pad_bottom), (0, 0)),\n",
    "                     mode='constant', constant_values=np.nan)\n",
    "\n",
    "    # 念のため長さを L に揃える\n",
    "    if arr.shape[0] != L:\n",
    "        if arr.shape[0] < L:\n",
    "            # 不足分を NaN で埋める\n",
    "            diff = L - arr.shape[0]\n",
    "            pad_bottom = np.full((diff, 1), np.nan)\n",
    "            arr = np.vstack([arr, pad_bottom])\n",
    "        else:\n",
    "            # 余分な行があれば切り詰め\n",
    "            arr = arr[:L, :]\n",
    "\n",
    "    return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b780ff-bf76-4943-9e26-c57796518163",
   "metadata": {},
   "source": [
    "## train洪水に対する予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f9d0a9-08f5-40ff-9161-7615c45f67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リードタイムの指定（何時間先の予測までを出力するか）\n",
    "lead_time = 240\n",
    "\n",
    "pred_train = []\n",
    "\n",
    "# trainデータについてのループ\n",
    "for i in range(train_idx.shape[0]): # 洪水期間ごとのループ\n",
    "    s = int(train_idx.iloc[i, 0]) - 1 - lead_time\n",
    "    e = int(train_idx.iloc[i, 1]) - 1\n",
    "\n",
    "    L = e - s + 1 # 洪水期間の長さ\n",
    "\n",
    "    pred_cols = [] # リスト変数として、予測を保存\n",
    "    pred_train_temp = []\n",
    "    pred_train_temp = d_all.iloc[s-1:e, 0:4].copy() # まずは洪水期間の[Time, Turbidity, Q, r]を抽出（Timeは予測開始時刻）(開始時刻にするため1時刻ずらす)\n",
    "    pred_train_temp[\"FloodID\"] = i + 1\n",
    "\n",
    "    X_raw_prev = None # 前時刻のinput保持用の変数\n",
    "\n",
    "    for j in range(lead_time): # リードタイムごとのループ\n",
    "    \n",
    "        if j == 0: # 1番初め（1時間先）の予測\n",
    "            cols_block = []\n",
    "            for col, lag in zip(input_cols, input_lags):\n",
    "                start = s - lag\n",
    "                end = e - lag\n",
    "                # lag分だけ前の行を取り出す\n",
    "                x_part = slice_L_nan(d_all, col, start, end, L)\n",
    "                cols_block.append(x_part)\n",
    "    \n",
    "            X_seg = np.hstack(cols_block) # (L, len(input_cols))\n",
    "            X_raw_prev = X_seg\n",
    "            X_seg_std = (X_seg - x_mean) / x_std # 標準化\n",
    "            X_seg_std = torch.from_numpy(X_seg_std.astype(np.float32)) # torch.Tensor型に\n",
    "            X_seg_std = X_seg_std.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_std = model(X_seg_std) # modelで予測\n",
    "\n",
    "            y_pred = y_pred_std.detach().cpu().numpy() * y_std + y_mean # 逆標準化\n",
    "\n",
    "            # 予測値が負の場合、0に置き換える\n",
    "            y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "            # 列名の指定\n",
    "            col_name = \"Tur(t+1)\"\n",
    "            # 得られた予測を記録\n",
    "            pred_cols.append(pd.Series(y_pred.flatten(), index=pred_train_temp.index, name=col_name))\n",
    "            \n",
    "            \n",
    "        else: # 2時間先以降の予測\n",
    "            cols_block = []\n",
    "            for col, lag in zip(input_cols, input_lags):\n",
    "                \n",
    "                if (col in output_cols) and (lag == 1): # inputがTur(t-1)のとき\n",
    "                    # 前時刻での予測を次の予測の入力値に\n",
    "                    x_part = y_pred.reshape(-1, 1)\n",
    "                    cols_block.append(x_part)\n",
    "\n",
    "                elif (col in output_cols) and (lag > 1): # inputがTur(t-2)以降の時\n",
    "                    # colが同じで、lagがlag+1のものを探す\n",
    "                    col_target = None\n",
    "                    for idx, (c2, l2) in enumerate(zip(input_cols, input_lags)):\n",
    "                        if (c2 == col) and (l2 == lag - 1):\n",
    "                            col_target = idx\n",
    "                            break  # 最初に見つかった時点で抜ける\n",
    "                    if col_target is None:\n",
    "                        raise ValueError(f\"対応する列がありません: col={col}, lag={lag}\")\n",
    "                    \n",
    "                    x_part = X_raw_prev[:, col_target:col_target+1]\n",
    "                    cols_block.append(x_part)\n",
    "\n",
    "\n",
    "                else: # inputがTur()以外の時\n",
    "                    # リードタイム分（j）ずらす\n",
    "                    start = max(0, s - lag + j)\n",
    "                    end = min(len(d_all) - 1, e - lag + j)\n",
    "                    x_part = slice_L_nan(d_all, col, start, end, L)\n",
    "                    cols_block.append(x_part)\n",
    "            \n",
    "            X_seg = np.hstack(cols_block) # (L, len(input_cols))\n",
    "            X_raw_prev = X_seg\n",
    "            X_seg_std = (X_seg - x_mean) / x_std # 標準化\n",
    "            X_seg_std = torch.from_numpy(X_seg_std.astype(np.float32)) # torch.Tensor型に\n",
    "            X_seg_std = X_seg_std.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_std = model(X_seg_std) # modelで予測\n",
    "\n",
    "            y_pred = y_pred_std.detach().cpu().numpy() * y_std + y_mean # 逆標準化\n",
    "\n",
    "            # 予測値が負の場合、0に置き換える\n",
    "            y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "            # 列名の指定\n",
    "            col_name = f\"Tur(t+{j+1})\"\n",
    "            # 得られた予測を記録\n",
    "            pred_cols.append(pd.Series(y_pred.flatten(), index=pred_train_temp.index, name=col_name))\n",
    "\n",
    "    pred_train_temp = pd.concat([pred_train_temp] + pred_cols, axis=1)\n",
    "    pred_train.append(pred_train_temp)\n",
    "\n",
    "# 1) 縦方向に結合（行結合）\n",
    "train_result = pd.concat(pred_train, axis=0, ignore_index=False)  # ignore_index=True で連番振り直し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbafa8d4-8ae7-4365-99e0-efd128820281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エクセルファイルに保存\n",
    "\n",
    "train_result.to_excel(train_result_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf894c5-378d-43c7-9dcf-0b799b6a60fd",
   "metadata": {},
   "source": [
    "## test洪水に対する予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4af6a2-2af0-4e8f-a7aa-ce786d25d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リードタイムの指定（何時間先の予測までを出力するか）\n",
    "lead_time = 240\n",
    "\n",
    "pred_test = []\n",
    "\n",
    "# testデータについてのループ\n",
    "for i in range(test_idx.shape[0]): # 洪水期間ごとのループ\n",
    "    s = int(test_idx.iloc[i, 0]) - 1 - lead_time\n",
    "    e = int(test_idx.iloc[i, 1]) - 1\n",
    "\n",
    "    L = e - s + 1 # 洪水期間の長さ\n",
    "\n",
    "    pred_cols = [] # リスト変数として、予測を保存\n",
    "    pred_test_temp = []\n",
    "    pred_test_temp = d_all.iloc[s-1:e, 0:4].copy() # まずは洪水期間の[Time, Turbidity, Q, r]を抽出（Timeは予測開始時刻）(開始時刻にするため1時刻ずらす)\n",
    "    pred_test_temp[\"FloodID\"] = i + 1\n",
    "\n",
    "    X_raw_prev = None # 前時刻のinput保持用の変数\n",
    "\n",
    "    for j in range(lead_time): # リードタイムごとのループ\n",
    "    \n",
    "        if j == 0: # 1番初め（1時間先）の予測\n",
    "            cols_block = []\n",
    "            for col, lag in zip(input_cols, input_lags):\n",
    "                start = s - lag\n",
    "                end = e - lag\n",
    "                # lag分だけ前の行を取り出す\n",
    "                x_part = slice_L_nan(d_all, col, start, end, L)\n",
    "                cols_block.append(x_part)\n",
    "    \n",
    "            X_seg = np.hstack(cols_block) # (L, len(input_cols))\n",
    "            X_raw_prev = X_seg\n",
    "            X_seg_std = (X_seg - x_mean) / x_std # 標準化\n",
    "            X_seg_std = torch.from_numpy(X_seg_std.astype(np.float32)) # torch.Tensor型に\n",
    "            X_seg_std = X_seg_std.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_std = model(X_seg_std) # modelで予測\n",
    "\n",
    "            y_pred = y_pred_std.detach().cpu().numpy() * y_std + y_mean # 逆標準化\n",
    "\n",
    "            # 予測値が負の場合、0に置き換える\n",
    "            y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "            # 列名の指定\n",
    "            col_name = \"Tur(t+1)\"\n",
    "            # 得られた予測を記録\n",
    "            pred_cols.append(pd.Series(y_pred.flatten(), index=pred_test_temp.index, name=col_name))\n",
    "            \n",
    "            \n",
    "        else: # 2時間先以降の予測\n",
    "            cols_block = []\n",
    "            for col, lag in zip(input_cols, input_lags):\n",
    "                \n",
    "                if (col in output_cols) and (lag == 1): # inputがTur(t-1)のとき\n",
    "                    # 前時刻での予測を次の予測の入力値に\n",
    "                    x_part = y_pred.reshape(-1, 1)\n",
    "                    cols_block.append(x_part)\n",
    "\n",
    "                elif (col in output_cols) and (lag > 1): # inputがTur(t-2)以降の時\n",
    "                    # colが同じで、lagがlag+1のものを探す\n",
    "                    col_target = None\n",
    "                    for idx, (c2, l2) in enumerate(zip(input_cols, input_lags)):\n",
    "                        if (c2 == col) and (l2 == lag - 1):\n",
    "                            col_target = idx\n",
    "                            break  # 最初に見つかった時点で抜ける\n",
    "                    if col_target is None:\n",
    "                        raise ValueError(f\"対応する列がありません: col={col}, lag={lag}\")\n",
    "                    \n",
    "                    x_part = X_raw_prev[:, col_target:col_target+1]\n",
    "                    cols_block.append(x_part)\n",
    "\n",
    "\n",
    "                else: # inputがTur()以外の時\n",
    "                    # リードタイム分（j）ずらす\n",
    "                    start = max(0, s - lag + j)\n",
    "                    end = min(len(d_all) - 1, e - lag + j)\n",
    "                    x_part = slice_L_nan(d_all, col, start, end, L)\n",
    "                    cols_block.append(x_part)\n",
    "            \n",
    "            X_seg = np.hstack(cols_block) # (L, len(input_cols))\n",
    "            X_raw_prev = X_seg\n",
    "            X_seg_std = (X_seg - x_mean) / x_std # 標準化\n",
    "            X_seg_std = torch.from_numpy(X_seg_std.astype(np.float32)) # torch.Tensor型に\n",
    "            X_seg_std = X_seg_std.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_std = model(X_seg_std) # modelで予測\n",
    "\n",
    "            y_pred = y_pred_std.detach().cpu().numpy() * y_std + y_mean # 逆標準化\n",
    "\n",
    "            # 予測値が負の場合、0に置き換える\n",
    "            y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "            # 列名の指定\n",
    "            col_name = f\"Tur(t+{j+1})\"\n",
    "            # 得られた予測を記録\n",
    "            pred_cols.append(pd.Series(y_pred.flatten(), index=pred_test_temp.index, name=col_name))\n",
    "\n",
    "    pred_test_temp = pd.concat([pred_test_temp] + pred_cols, axis=1)\n",
    "    pred_test.append(pred_test_temp)\n",
    "\n",
    "# 1) 縦方向に結合（行結合）\n",
    "test_result = pd.concat(pred_test, axis=0, ignore_index=False)  # ignore_index=True で連番振り直し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fffae1d-5c04-48c9-bd8c-233458932a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エクセルファイルに保存\n",
    "test_result.to_excel(test_result_path, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3014ec-d035-43ae-9fa2-0e80d94cfac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
