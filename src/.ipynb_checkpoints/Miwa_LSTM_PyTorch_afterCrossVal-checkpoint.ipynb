{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dca737e-9bc0-45e1-941a-a64358a97480",
   "metadata": {},
   "source": [
    "### Cross Validation後にモデルを学習・保存する\n",
    "### テストデータで、モデルの精度を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9091fe2f-c27c-46b6-a766-6166c558b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import os, copy\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937600f8-c2f8-4dd0-b9e4-06c72b312983",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da89cb5-0107-44ab-a773-b3cc4b7e8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ユーザ設定 ======\n",
    "\n",
    "excel_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\CrossVal_5\\Miwa_hourlyAve_for_LSTM_CrossVal_5.xlsx\"\n",
    "flood_idx_path_train = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\CrossVal_5\\Miwa_flood_idx_train.xlsx\"\n",
    "flood_idx_path_test = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\data\\Miwa_LSTM_Data\\CrossVal_5\\Miwa_flood_idx_test.xlsx\"\n",
    "\n",
    "\n",
    "# 列番号（0始まり）で指定\n",
    "enc_cols   = [5, 3, 4]      # エンコーダ入力の列番号（例：3変数）\n",
    "dec_cols   = [5, 3]  # デコーダ入力の列番号（例：2変数）\n",
    "y_cols      = [4]          # 出力（目的変数）の列番号（例：1変数）\n",
    "\n",
    "Te = 72    # エンコーダのタイムステップ長\n",
    "Td = 240   # デコーダのタイムステップ長\n",
    "\n",
    "\n",
    "# 洪水区間（1始まり行番号で指定してOK。Python内部で0始まりに直す）\n",
    "df_ranges_train = pd.read_excel(flood_idx_path_train, header=0)\n",
    "flood_ranges_train_1based = [tuple(x) for x in df_ranges_train.to_numpy()]\n",
    "\n",
    "# 0-basedに変換（pandasは0-based）\n",
    "flood_ranges_train = [(s-1, e-1) for (s, e) in flood_ranges_train_1based]\n",
    "\n",
    "\n",
    "\n",
    "# ====== 読み込み ======\n",
    "df = pd.read_excel(excel_path, header=0)\n",
    "\n",
    "# 必要列だけ抽出（順番固定）\n",
    "use_cols = enc_cols + dec_cols + y_cols\n",
    "data = df.iloc[:, use_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edb06ac-6023-401c-85bd-4ebdccf7c156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---平均（train）----\n",
      "CumRain_24h     14.273778\n",
      "Qin(m3/s)       46.917530\n",
      "Tur(ppm)       351.369040\n",
      "CumRain_24h     14.273778\n",
      "Qin(m3/s)       46.917530\n",
      "Tur(ppm)       351.369040\n",
      "dtype: float64\n",
      "----標準偏差（train）----\n",
      "CumRain_24h     24.155857\n",
      "Qin(m3/s)       47.874068\n",
      "Tur(ppm)       933.516769\n",
      "CumRain_24h     24.155857\n",
      "Qin(m3/s)       47.874068\n",
      "Tur(ppm)       933.516769\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ====== 標準化：train dataから平均・標準偏差を算出 ======\n",
    "\n",
    "flood_data_train_parts = [data.iloc[s:e+1, :] for (s, e) in flood_ranges_train]\n",
    "flood_data_train = pd.concat(flood_data_train_parts, axis=0)\n",
    "\n",
    "mean = flood_data_train.mean(numeric_only=True)\n",
    "std  = flood_data_train.std(numeric_only=True).replace(0, 1.0)\n",
    "data_norm = (data - mean) / std # 全期間のデータを標準化\n",
    "\n",
    "print('---平均（train）----')\n",
    "print(mean)\n",
    "\n",
    "print('----標準偏差（train）----')\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfdf953-dcc9-4127-b835-88e1bacfa87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, triplets):\n",
    "        self.triplets = triplets  # list of (enc_X, dec_X, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triplets[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc_seqs, dec_seqs, ys = zip(*batch)\n",
    "    # ここでは全サンプル同一長さ前提（Te/Td固定）なので単純stack\n",
    "    enc_x = torch.stack(enc_seqs, dim=0)  # [B, Te, Fe]\n",
    "    dec_x = torch.stack(dec_seqs, dim=0)  # [B, Td, Fd]\n",
    "    y     = torch.stack(ys,       dim=0)  # [B, Td, Fo]\n",
    "    \n",
    "    # マスク（将来可変長のときに使う）\n",
    "    B, Td, _ = y.shape\n",
    "    mask = torch.ones(B, Td, 1, dtype=torch.float32)\n",
    "    return enc_x, dec_x, y, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9d7bf4-e143-4b1c-a82b-f3e1eb110171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 洪水区間ごとにスライド窓でサンプル作成（train） ======\n",
    "\n",
    "samples_train = []  # list of (enc_X: [Te, Fe], dec_X: [Td, Fd], y: [Td, Fo])\n",
    "Fe, Fd, Fo = len(enc_cols), len(dec_cols), len(y_cols)\n",
    "\n",
    "for (s, e) in flood_ranges_train:\n",
    "    seg = data_norm.iloc[s:e+1]  # 区間データ（両端含む）\n",
    "    n = len(seg)\n",
    "    if n < Te + Td: # 1サンプルは Te + Tdの長さが必要\n",
    "        continue\n",
    "        \n",
    "    for start in range(0, n - (Te + Td) + 1):\n",
    "        enc_window = seg.iloc[start : start + Te]\n",
    "        dec_window = seg.iloc[start + Te : start + Te + Td]\n",
    "        # テンソル化\n",
    "        enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))     # [Te, Fe]\n",
    "        dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))     # [Td, Fd]\n",
    "        y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))      # [Td, Fo]\n",
    "        samples_train.append((enc_X, dec_X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd238b74-fadb-4996-9fa5-c95d338e1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 洪水区間ごとにスライド窓でサンプル作成（test） ======\n",
    "\n",
    "df_ranges_test = pd.read_excel(flood_idx_path_test, header=0)\n",
    "flood_ranges_test_1based = [tuple(x) for x in df_ranges_test.to_numpy()]\n",
    "flood_ranges_test = [(s-1, e-1) for (s, e) in flood_ranges_test_1based]\n",
    "\n",
    "\n",
    "samples_test = []  # list of (enc_X: [Te, Fe], dec_X: [Td, Fd], y: [Td, Fo])\n",
    "Fe, Fd, Fo = len(enc_cols), len(dec_cols), len(y_cols)\n",
    "\n",
    "for (s, e) in flood_ranges_test:\n",
    "    seg = data_norm.iloc[s:e+1]  # 区間データ（両端含む）\n",
    "    n = len(seg)\n",
    "    if n < Te + Td: # 1サンプルは Te + Tdの長さが必要\n",
    "        continue\n",
    "        \n",
    "    for start in range(0, n - (Te + Td) + 1):\n",
    "        enc_window = seg.iloc[start : start + Te]\n",
    "        dec_window = seg.iloc[start + Te : start + Te + Td]\n",
    "        # テンソル化\n",
    "        enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))     # [Te, Fe]\n",
    "        dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))     # [Td, Fd]\n",
    "        y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))      # [Td, Fo]\n",
    "        samples_test.append((enc_X, dec_X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ea472-5810-4499-9fc8-c68929d31744",
   "metadata": {},
   "source": [
    "## 損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b5587eb-7a1f-4b36-8609-281d28dce28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(batch):\n",
    "    \"\"\"collate_fnの戻り値が (enc,dec,y) か (enc,dec,y,mask) のどちらでも対応\"\"\"\n",
    "    if len(batch) == 3:\n",
    "        enc_x, dec_x, y = batch\n",
    "        mask = torch.ones_like(y[..., :1])  # [B,Td,1]\n",
    "    elif len(batch) == 4:\n",
    "        enc_x, dec_x, y, mask = batch\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected batch format\")\n",
    "    return enc_x.to(device), dec_x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "def masked_mse(pred, target, mask):\n",
    "    # pred/target: [B,T,Out], mask: [B,T,1] (1=valid, 0=pad)\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask\n",
    "    denom = mask.sum(dim=(1,2)).clamp_min(1.0)  # per-sample\n",
    "    per_sample = diff2.sum(dim=(1,2)) / denom\n",
    "    return per_sample.mean()\n",
    "\n",
    "def masked_rmse(pred, target, mask):\n",
    "    return torch.sqrt(masked_mse(pred, target, mask))\n",
    "\n",
    "def masked_r2(pred, target, mask):\n",
    "    # R² = 1 - SSE/SST, マスク版\n",
    "    mean = (target * mask).sum(dim=(1,2), keepdim=True) / mask.sum(dim=(1,2), keepdim=True).clamp_min(1.0)\n",
    "    sse = ((pred - target) ** 2 * mask).sum(dim=(1,2))\n",
    "    sst = ((target - mean) ** 2 * mask).sum(dim=(1,2)).clamp_min(1e-12)\n",
    "    r2  = 1.0 - sse / sst\n",
    "    return r2.mean()\n",
    "\n",
    "def masked_corr(pred, target, mask):\n",
    "    # pred/target: [B,T,Out], mask: [B,T,1]\n",
    "    pred = pred * mask\n",
    "    target = target * mask\n",
    "    valid = mask.sum(dim=(1,2)).clamp_min(1.0)\n",
    "\n",
    "    # 平均\n",
    "    mean_pred = pred.sum(dim=(1,2)) / valid\n",
    "    mean_target = target.sum(dim=(1,2)) / valid\n",
    "\n",
    "    # 偏差\n",
    "    diff_pred = (pred - mean_pred.view(-1,1,1)) * mask\n",
    "    diff_target = (target - mean_target.view(-1,1,1)) * mask\n",
    "\n",
    "    # 共分散と分散\n",
    "    cov = (diff_pred * diff_target).sum(dim=(1,2)) / valid\n",
    "    var_pred = (diff_pred**2).sum(dim=(1,2)) / valid\n",
    "    var_target = (diff_target**2).sum(dim=(1,2)) / valid\n",
    "\n",
    "    corr = cov / (torch.sqrt(var_pred * var_target) + 1e-12)\n",
    "    return corr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed2a7c-8522-40ea-8d12-431bc6175104",
   "metadata": {},
   "source": [
    "## モデル定義（Seq2SeqLSTM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40758f9-0c2b-406c-a25d-d2abfe40ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateBridge(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoderの(h, c)をDecoder初期状態へ写像するブリッジ。\n",
    "    - 層数/隠れ次元が異なってもOK\n",
    "    - bridge_mode:\n",
    "        - \"zero_pad\":  層合わせ=0埋め or 切り落とし、隠れ次元は線形射影\n",
    "        - \"repeat_top\":層合わせ=最上層の繰り返し/切り落とし、隠れ次元は線形射影\n",
    "        - \"linear_stack\": [B, L_enc, H_enc] -> 線形で [B, L_dec, H_dec] へ（層方向も学習で混合）\n",
    "\n",
    "    - enc_layers: エンコーダの層の深さ\n",
    "    - dec_layers: デコーダの層の深さ\n",
    "    - enc_hidden: エンコーダのノード数\n",
    "    - dec_hidden: デコーダのノード数\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_layers, dec_layers, enc_hidden, dec_hidden, mode=\"zero_pad\"):\n",
    "        super().__init__()\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.enc_hidden = enc_hidden\n",
    "        self.dec_hidden = dec_hidden\n",
    "        self.mode = mode\n",
    "\n",
    "        # 隠れ次元の変換（h/c共用）\n",
    "        if mode in (\"zero_pad\", \"repeat_top\"):\n",
    "            self.proj = nn.Linear(enc_hidden, dec_hidden, bias=True)\n",
    "        elif mode == \"linear_stack\":\n",
    "            # 層方向もまとめて線形変換\n",
    "            self.proj_h = nn.Linear(enc_layers * enc_hidden, dec_layers * dec_hidden, bias=True)\n",
    "            self.proj_c = nn.Linear(enc_layers * enc_hidden, dec_layers * dec_hidden, bias=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown bridge mode: {mode}\")\n",
    "\n",
    "    def _match_layers(self, x, how=\"zero_pad\"):\n",
    "        \"\"\"\n",
    "        x: [L_enc, B, H_enc] を層数だけ合わせる（隠れ次元は未変換）\n",
    "        return: [L_dec, B, H_enc]\n",
    "\n",
    "        B: バッチサイズ\n",
    "        \"\"\"\n",
    "        L_enc, B, H = x.shape\n",
    "        L_dec = self.dec_layers\n",
    "\n",
    "        if L_dec == L_enc:\n",
    "            return x\n",
    "\n",
    "        if L_dec < L_enc:\n",
    "            # 上位層を優先して切り落とす（直観的には最上層が一番抽象的）\n",
    "            return x[:L_dec, :, :]\n",
    "\n",
    "        # L_dec > L_enc の場合\n",
    "        pad_count = L_dec - L_enc\n",
    "        if how == \"repeat_top\":\n",
    "            top = x[-1:, :, :].expand(pad_count, B, H)  # 最上層を複製\n",
    "            return torch.cat([x, top], dim=0)\n",
    "        else:  # zero_pad\n",
    "            pad = x.new_zeros(pad_count, B, H)\n",
    "            return torch.cat([x, pad], dim=0)\n",
    "\n",
    "    def forward(self, h_enc, c_enc):\n",
    "        \"\"\"\n",
    "        h_enc, c_enc: [L_enc, B, H_enc]\n",
    "        返り値: (h0_dec, c0_dec) それぞれ [L_dec, B, H_dec]\n",
    "        \"\"\"\n",
    "        if self.mode in (\"zero_pad\", \"repeat_top\"):\n",
    "            # 層合わせ（まだ enc_hidden 次元のまま）\n",
    "            h = self._match_layers(h_enc, \"repeat_top\" if self.mode==\"repeat_top\" else \"zero_pad\")\n",
    "            c = self._match_layers(c_enc, \"repeat_top\" if self.mode==\"repeat_top\" else \"zero_pad\")\n",
    "            # 次元射影\n",
    "            L, B, H = h.shape\n",
    "            h = self.proj(h)  # broadcasting: [L,B,H_enc]->[L,B,H_dec]\n",
    "            c = self.proj(c)\n",
    "            return h, c\n",
    "\n",
    "        else:  # linear_stack\n",
    "            # [L_enc,B,H_enc] -> [B, L_enc*H_enc]\n",
    "            L_enc, B, H_enc = h_enc.shape\n",
    "            flat_h = h_enc.transpose(0,1).reshape(B, L_enc*H_enc)\n",
    "            flat_c = c_enc.transpose(0,1).reshape(B, L_enc*H_enc)\n",
    "            # 線形写像\n",
    "            out_h = self.proj_h(flat_h)  # [B, L_dec*H_dec]\n",
    "            out_c = self.proj_c(flat_c)\n",
    "            # [L_dec,B,H_dec] に戻す\n",
    "            L_dec, H_dec = self.dec_layers, self.dec_hidden\n",
    "            h = out_h.view(B, L_dec, H_dec).transpose(0,1).contiguous()\n",
    "            c = out_c.view(B, L_dec, H_dec).transpose(0,1).contiguous()\n",
    "            return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2beb4d-b16f-45af-9da5-934293ce1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_enc: int, # エンコーダ入力変数の数\n",
    "        in_dec: int, # デコーダ入力変数の数\n",
    "        out_dim: int, # 出力変数の数\n",
    "        enc_hidden: int = 128,\n",
    "        dec_hidden: int = 128,\n",
    "        enc_layers: int = 2,\n",
    "        dec_layers: int = 3,\n",
    "        bridge_mode: str = \"zero_pad\",  # \"zero_pad\" | \"repeat_top\" | \"linear_stack\"\n",
    "        dropout: float = 0.0, # LSTMの層間ドロップアウト率\n",
    "        bidirectional_enc: bool = False,  # エンコーダのみ双方向にするかどうか（必要ならEncoderを双方向にも）\n",
    "        head_activation=\"relu\", # 活性化関数\n",
    "        use_conv: bool = True,\n",
    "        conv_kernel: int = 3,           # 時間方向のカーネル幅（奇数推奨）\n",
    "        conv_channels: int = None,      # 省略時は dec_hidden を維持\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional_enc = bidirectional_enc\n",
    "        enc_dir = 2 if bidirectional_enc else 1\n",
    "        enc_hidden_eff = enc_hidden * enc_dir  # 双方向なら出力次元が倍\n",
    "\n",
    "        self.use_conv = bool(use_conv)\n",
    "\n",
    "        self.enc = nn.LSTM(\n",
    "            input_size=in_enc,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=enc_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if enc_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional_enc\n",
    "        )\n",
    "\n",
    "        self.dec = nn.LSTM(\n",
    "            input_size=in_dec,\n",
    "            hidden_size=dec_hidden,\n",
    "            num_layers=dec_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dec_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Encoderが双方向のときは、(h_fwd, h_bwd) を結合した次元 enc_hidden_eff を\n",
    "        # Decoder hidden 次元へ写像する必要がある\n",
    "        self.bridge = StateBridge(\n",
    "            enc_layers=enc_layers * enc_dir,\n",
    "            dec_layers=dec_layers,\n",
    "            enc_hidden=enc_hidden,\n",
    "            dec_hidden=dec_hidden,\n",
    "            mode=bridge_mode\n",
    "        ) if (enc_layers != dec_layers or enc_dir != 1 or enc_hidden != dec_hidden or bridge_mode==\"linear_stack\") else None\n",
    "\n",
    "        \n",
    "        # ---------- 中間層（1次元畳み込み） ----------\n",
    "        if conv_channels is None:\n",
    "            conv_channels = dec_hidden\n",
    "        self.conv_channels = conv_channels\n",
    "        if self.use_conv:\n",
    "            # Conv1dは [B, C=特徴, T=時間] を受け取るので後で転置する\n",
    "            padding = conv_kernel // 2  # SAME相当（奇数カーネル推奨）\n",
    "            self.conv1d = nn.Conv1d(\n",
    "                in_channels=dec_hidden,\n",
    "                out_channels=conv_channels,\n",
    "                kernel_size=conv_kernel,\n",
    "                padding=padding\n",
    "            )\n",
    "            self.conv_act = {\n",
    "                \"identity\": nn.Identity(),\n",
    "                \"relu\": nn.ReLU(),\n",
    "                \"tanh\": nn.Tanh(),\n",
    "                \"sigmoid\": nn.Sigmoid(),\n",
    "            }.get(head_activation, nn.ReLU())\n",
    "            self.conv_dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "            # 出力ヘッドは conv_channels → out_dim\n",
    "            self.head = nn.Linear(conv_channels, out_dim)\n",
    "            # ここでの self.act はヘッド直前では使わない（Conv後に適用済み）\n",
    "            self.act = nn.Identity()\n",
    "        else:\n",
    "            # 畳み込みを使わない場合は dec_hidden → out_dim\n",
    "            self.head = nn.Linear(dec_hidden, out_dim)\n",
    "            self.act = {\n",
    "                \"identity\": nn.Identity(),\n",
    "                \"relu\": nn.ReLU(),\n",
    "                \"tanh\": nn.Tanh(),\n",
    "                \"sigmoid\": nn.Sigmoid(),\n",
    "            }[head_activation]\n",
    "\n",
    "    def _extract_final_states(self, out, hc):\n",
    "        \"\"\"\n",
    "        LSTMの出力から (h_T, c_T) を取り出して成形。\n",
    "        双方向Encoderの場合は各層ごとに [fwd, bwd] を層方向に並べる。\n",
    "        \"\"\"\n",
    "        h, c = hc  # [num_layers * num_directions, B, H]\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        \"\"\"\n",
    "        enc_x: [B, Te, in_enc]\n",
    "        dec_x: [B, Td, in_dec]\n",
    "        return: yhat [B, Td, out_dim]\n",
    "        \"\"\"\n",
    "        # ----- Encoder -----\n",
    "        _, (h_T, c_T) = self.enc(enc_x)  # h_T,c_T: [L_enc * dir, B, H_enc]\n",
    "\n",
    "        # ----- Bridge -----\n",
    "        if self.bridge is not None:\n",
    "            h0_dec, c0_dec = self.bridge(h_T, c_T)  # [L_dec, B, H_dec]\n",
    "        else:\n",
    "            h0_dec, c0_dec = h_T, c_T\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        dec_out, _ = self.dec(dec_x, (h0_dec, c0_dec))  # [B, Td, H_dec]\n",
    "\n",
    "        if self.use_conv:\n",
    "            # 時間方向のConv1d: [B, H_dec, Td] -> Conv -> [B, C, Td] -> [B, Td, C]\n",
    "            x = dec_out.transpose(1, 2)             # [B, H_dec, Td]\n",
    "            x = self.conv1d(x)                      # [B, conv_channels, Td]\n",
    "            x = self.conv_act(x)\n",
    "            x = self.conv_dropout(x)\n",
    "            x = x.transpose(1, 2)                   # [B, Td, conv_channels]\n",
    "            yhat = self.head(x)                     # [B, Td, out_dim]\n",
    "        else:\n",
    "            # 従来パス：活性化→線形\n",
    "            yhat = self.head(self.act(dec_out))     # [B, Td, out_dim]\n",
    "\n",
    "        return yhat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edde7056-145e-4a01-ba89-e5c27c9ce0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重み初期化用の関数\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # 例: Xavier（正規分布）\n",
    "        # init.xavier_normal_(m.weight)\n",
    "        # ReLU系のLinearならHeにしたい場合:\n",
    "        init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    elif isinstance(m, nn.Conv1d):\n",
    "        # Conv1d も Linear と同様に初期化可能\n",
    "        # 例: Xavier（正規分布）\n",
    "        # init.xavier_normal_(m.weight)\n",
    "        # ReLU系のLinearならHeにしたい場合:\n",
    "        init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        # LSTM の場合は named_parameters() で内部ゲートを個別に初期化\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                # 入力→隠れ：Xavier が安定\n",
    "                with torch.no_grad():\n",
    "                    init.xavier_uniform_(param)\n",
    "            elif \"weight_hh\" in name:\n",
    "                # 隠れ→隠れ：Orthogonal が定番\n",
    "                with torch.no_grad():\n",
    "                    init.orthogonal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                with torch.no_grad():\n",
    "                    init.zeros_(param)\n",
    "                    # 必要なら忘却ゲートバイアスを +1 初期化することも可能\n",
    "                    # hidden_size = param.shape[0] // 4\n",
    "                    # param[hidden_size:2*hidden_size] = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3408b-f127-4019-8c6d-b76c86109395",
   "metadata": {},
   "source": [
    "## ハイパーパラメータの設定\n",
    "### ※保存Excelファイルのパスも設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0994e29f-978e-42b8-b677-a163f4e2adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = {\n",
    "    # Model\n",
    "    \"in_enc\": 3,\n",
    "    \"in_dec\": 2,\n",
    "    \"out_dim\": 1,\n",
    "    \"enc_hidden\": 16, # 【要変更】\n",
    "    \"dec_hidden\": 16, # 【要変更】\n",
    "    \"enc_layers\": 1, # 【要変更】\n",
    "    \"dec_layers\": 1, # 【要変更】\n",
    "    \"bridge_mode\": \"zero_pad\",   # \"zero_pad\" | \"repeat_top\" | \"linear_stack\"\n",
    "    \"batch_size\": 256, # 【要変更】\n",
    "    \"dropout\": 0.1,\n",
    "    \"bidirectional_enc\": False,\n",
    "    \"head_activation\": \"relu\", # \"identity\", \"relu\", \"tanh\", \"sigmoid\" など 【要変更】\n",
    "    # Train\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0.0, # L2正規化係数。0なら無効\n",
    "    \"grad_clip\": 1.0, # 勾配クリッピングの閾値。０かNoneなら無効\n",
    "    \"print_every\": 1, # 学習の進捗を何エポックごとに出力するか\n",
    "    \"patience\": 5, # early stopping\n",
    "    \"use_conv\": True # 畳み込み層を使うかどうか【要変更】\n",
    "}\n",
    "\n",
    "save_dir = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\"\n",
    "save_path_excel = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\CrossVal_5_result_final.xlsx\" # 【要変更】\n",
    "# save_path_model = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_2/CrossVal_2_result_final_model\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0f506-bf20-4b7e-aa9b-8b16596c1663",
   "metadata": {},
   "source": [
    "## 学習関数・評価関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf43fea-db77-40f9-956b-d61648aa0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習関数・評価関数の定義\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        enc_x, dec_x, y, mask = split_batch(batch)\n",
    "        yhat = model(enc_x, dec_x)\n",
    "        loss = masked_mse(yhat, y, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if cfg[\"grad_clip\"]:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = enc_x.size(0) # バッチサイズ\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "    return total_loss / max(n,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0; total_rmse = 0.0; total_r2 = 0.0; total_corr = 0.0; n = 0\n",
    "    for batch in loader:\n",
    "        enc_x, dec_x, y, mask = split_batch(batch)\n",
    "        yhat = model(enc_x, dec_x)\n",
    "\n",
    "        loss = masked_mse(yhat, y, mask)\n",
    "        rmse = masked_rmse(yhat, y, mask)\n",
    "        r2   = masked_r2(yhat, y, mask)\n",
    "        corr = masked_corr(yhat, y, mask)\n",
    "\n",
    "        bs = enc_x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_rmse += rmse.item() * bs\n",
    "        total_r2   += r2.item() * bs\n",
    "        total_corr += corr.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(n,1),\n",
    "        \"rmse\": total_rmse / max(n,1),\n",
    "        \"r2\":   total_r2   / max(n,1),\n",
    "        \"corr\": total_corr / max(n,1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab44b3b2-37dd-48f9-b88c-67e9f88ed16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_standardize_y_by_index(y_std, mean, std, y_pos):\n",
    "    \"\"\"\n",
    "    y_std:  標準化スケールの出力テンソル [B, T, Fo]\n",
    "    mean, std: pandas.Series（学習時にfitしたもの。index=列名）\n",
    "    y_pos: 出力列の「列番号（位置）」リスト（例: [Fe+Fd, Fe+Fd+1, ...]）\n",
    "    return: 元スケールの y [B, T, Fo]\n",
    "    \"\"\"\n",
    "    m = torch.tensor(mean.iloc[y_pos].to_numpy(dtype=float),\n",
    "                     dtype=y_std.dtype, device=y_std.device).view(1, 1, -1)\n",
    "    s = torch.tensor(std.iloc[y_pos].to_numpy(dtype=float),\n",
    "                     dtype=y_std.dtype, device=y_std.device).view(1, 1, -1)\n",
    "    return y_std * s + m\n",
    "\n",
    "\n",
    "def masked_corr(pred, target, mask, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    マスク付きピアソン相関係数（バッチ平均）\n",
    "    pred/target: [B, T, Fo], mask: [B, T, 1]（1=有効, 0=無効）\n",
    "    返り値: スカラー（バッチ平均の相関）\n",
    "    \"\"\"\n",
    "    # 有効点数（サンプルごと）\n",
    "    valid = mask.sum(dim=(1, 2)).clamp_min(1.0)  # [B]\n",
    "\n",
    "    # 平均（サンプルごと）\n",
    "    mean_p = (pred * mask).sum(dim=(1, 2), keepdim=True) / valid.view(-1, 1, 1)\n",
    "    mean_t = (target * mask).sum(dim=(1, 2), keepdim=True) / valid.view(-1, 1, 1)\n",
    "\n",
    "    # 偏差\n",
    "    dp = (pred - mean_p) * mask\n",
    "    dt = (target - mean_t) * mask\n",
    "\n",
    "    # 共分散・分散（サンプルごと）\n",
    "    cov = dp.mul(dt).sum(dim=(1, 2)) / valid        # [B]\n",
    "    var_p = dp.pow(2).sum(dim=(1, 2)) / valid       # [B]\n",
    "    var_t = dt.pow(2).sum(dim=(1, 2)) / valid       # [B]\n",
    "\n",
    "    corr = cov / (torch.sqrt(var_p * var_t) + eps)  # [B]\n",
    "    return corr.mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_original_scale_by_index(model, loader, mean, std, data_columns, y_pos):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 厳密集計用\n",
    "    total_sse = 0.0   # 全有効点での誤差二乗和\n",
    "    total_cnt = 0.0   # 全有効点数（マスク=1の総数）\n",
    "    total_r2  = 0.0   # R² のバッチ加重平均用\n",
    "    total_corr = 0.0  # 相関R のバッチ加重平均用\n",
    "    n = 0             # サンプル数（バッチ内のBの合計）\n",
    "\n",
    "    for batch in loader:\n",
    "        if len(batch) == 3:\n",
    "            enc_x, dec_x, y_std = batch\n",
    "            mask = torch.ones_like(y_std[..., :1])\n",
    "        elif len(batch) == 4:\n",
    "            enc_x, dec_x, y_std, mask = batch\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected batch format\")\n",
    "\n",
    "        # 統一デバイス・dtype\n",
    "        enc_x = enc_x.to(device=device, dtype=torch.float32)\n",
    "        dec_x = dec_x.to(device=device, dtype=torch.float32)\n",
    "        y_std = y_std.to(device=device, dtype=torch.float32)\n",
    "        mask  = mask.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        # 予測（標準化スケール）\n",
    "        yhat_std = model(enc_x, dec_x)\n",
    "\n",
    "        # 出力だけ逆標準化\n",
    "        yhat = inverse_standardize_y_by_index(yhat_std, mean, std, y_pos)\n",
    "        y    = inverse_standardize_y_by_index(y_std,     mean, std, y_pos)\n",
    "\n",
    "        # 厳密RMSE用：SSEと有効点数を直接合算\n",
    "        sse_batch = ((yhat - y) ** 2 * mask).sum().item()\n",
    "        cnt_batch = mask.sum().item()\n",
    "        total_sse += sse_batch\n",
    "        total_cnt += max(cnt_batch, 1.0)\n",
    "\n",
    "        # R² と 相関R はサンプル数で加重平均\n",
    "        r2    = masked_r2(yhat, y, mask).item()\n",
    "        corr  = masked_corr(yhat, y, mask).item()\n",
    "        bs = enc_x.size(0)\n",
    "        total_r2   += r2   * bs\n",
    "        total_corr += corr * bs\n",
    "        n += bs\n",
    "\n",
    "    mse  = total_sse / max(total_cnt, 1.0)\n",
    "    rmse = mse ** 0.5\n",
    "    r2   = total_r2   / max(n, 1)\n",
    "    corr = total_corr / max(n, 1)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"r2\": r2, \"corr\": corr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6d72d-79ff-4ea1-9312-8554f3f15aa2",
   "metadata": {},
   "source": [
    "## 学習（シード値を変えて5回学習。結果を保存する）\n",
    "### ※train期間のうちランダムに85%を学習、15%を検証に分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee1c488-6305-4c96-8f63-dd2dd6e519ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] train_loss=0.9904 | val_loss=0.8325 val_rmse=0.9123 val_r2=-45.6097val_corr=0.6684\n",
      "[2/100] train_loss=0.6262 | val_loss=0.5065 val_rmse=0.7116 val_r2=-10.4074val_corr=0.7614\n",
      "[3/100] train_loss=0.4087 | val_loss=0.3770 val_rmse=0.6140 val_r2=-13.5184val_corr=0.7655\n",
      "[4/100] train_loss=0.3454 | val_loss=0.3278 val_rmse=0.5725 val_r2=-11.3045val_corr=0.7614\n",
      "[5/100] train_loss=0.3094 | val_loss=0.2789 val_rmse=0.5281 val_r2=-6.2643val_corr=0.7811\n",
      "[6/100] train_loss=0.2708 | val_loss=0.2391 val_rmse=0.4890 val_r2=-8.9562val_corr=0.7896\n",
      "[7/100] train_loss=0.2435 | val_loss=0.2159 val_rmse=0.4647 val_r2=-13.7078val_corr=0.7954\n",
      "[8/100] train_loss=0.2279 | val_loss=0.2007 val_rmse=0.4480 val_r2=-11.1973val_corr=0.8025\n",
      "[9/100] train_loss=0.2115 | val_loss=0.1819 val_rmse=0.4265 val_r2=-8.0405val_corr=0.8102\n",
      "[10/100] train_loss=0.2003 | val_loss=0.1710 val_rmse=0.4135 val_r2=-8.5039val_corr=0.8191\n",
      "[11/100] train_loss=0.1858 | val_loss=0.1582 val_rmse=0.3977 val_r2=-7.7079val_corr=0.8191\n",
      "[12/100] train_loss=0.1761 | val_loss=0.1704 val_rmse=0.4127 val_r2=-8.3980val_corr=0.8179\n",
      "[13/100] train_loss=0.1748 | val_loss=0.1472 val_rmse=0.3836 val_r2=-10.2119val_corr=0.8202\n",
      "[14/100] train_loss=0.1622 | val_loss=0.1393 val_rmse=0.3732 val_r2=-13.6794val_corr=0.8246\n",
      "[15/100] train_loss=0.1539 | val_loss=0.1277 val_rmse=0.3573 val_r2=-14.0366val_corr=0.8265\n",
      "[16/100] train_loss=0.1472 | val_loss=0.1251 val_rmse=0.3537 val_r2=-13.7779val_corr=0.8271\n",
      "[17/100] train_loss=0.1430 | val_loss=0.1189 val_rmse=0.3448 val_r2=-11.6254val_corr=0.8299\n",
      "[18/100] train_loss=0.1374 | val_loss=0.1157 val_rmse=0.3401 val_r2=-11.4222val_corr=0.8318\n",
      "[19/100] train_loss=0.1329 | val_loss=0.1130 val_rmse=0.3361 val_r2=-12.6334val_corr=0.8330\n",
      "[20/100] train_loss=0.1299 | val_loss=0.1080 val_rmse=0.3285 val_r2=-12.4483val_corr=0.8331\n",
      "[21/100] train_loss=0.1255 | val_loss=0.1042 val_rmse=0.3227 val_r2=-12.1102val_corr=0.8363\n",
      "[22/100] train_loss=0.1221 | val_loss=0.0999 val_rmse=0.3161 val_r2=-11.9322val_corr=0.8376\n",
      "[23/100] train_loss=0.1193 | val_loss=0.0994 val_rmse=0.3152 val_r2=-8.8357val_corr=0.8383\n",
      "[24/100] train_loss=0.1177 | val_loss=0.0980 val_rmse=0.3130 val_r2=-7.4303val_corr=0.8382\n",
      "[25/100] train_loss=0.1226 | val_loss=0.1056 val_rmse=0.3248 val_r2=-7.6230val_corr=0.8386\n",
      "[26/100] train_loss=0.1199 | val_loss=0.1002 val_rmse=0.3164 val_r2=-7.1270val_corr=0.8351\n",
      "[27/100] train_loss=0.1288 | val_loss=0.0996 val_rmse=0.3155 val_r2=-6.2439val_corr=0.8342\n",
      "[28/100] train_loss=0.1152 | val_loss=0.0924 val_rmse=0.3040 val_r2=-9.6201val_corr=0.8453\n",
      "[29/100] train_loss=0.1099 | val_loss=0.0865 val_rmse=0.2940 val_r2=-6.6102val_corr=0.8457\n",
      "[30/100] train_loss=0.1056 | val_loss=0.0851 val_rmse=0.2917 val_r2=-5.2272val_corr=0.8499\n",
      "[31/100] train_loss=0.1036 | val_loss=0.0834 val_rmse=0.2888 val_r2=-6.5752val_corr=0.8448\n",
      "[32/100] train_loss=0.1004 | val_loss=0.0814 val_rmse=0.2853 val_r2=-9.9470val_corr=0.8419\n",
      "[33/100] train_loss=0.0976 | val_loss=0.0782 val_rmse=0.2796 val_r2=-10.9014val_corr=0.8464\n",
      "[34/100] train_loss=0.0958 | val_loss=0.0791 val_rmse=0.2812 val_r2=-12.4261val_corr=0.8269\n",
      "[35/100] train_loss=0.0962 | val_loss=0.0753 val_rmse=0.2742 val_r2=-9.6714val_corr=0.8440\n",
      "[36/100] train_loss=0.0929 | val_loss=0.0719 val_rmse=0.2681 val_r2=-8.5236val_corr=0.8465\n",
      "[37/100] train_loss=0.0935 | val_loss=0.0728 val_rmse=0.2698 val_r2=-12.0180val_corr=0.8450\n",
      "[38/100] train_loss=0.0927 | val_loss=0.0731 val_rmse=0.2702 val_r2=-14.2173val_corr=0.8475\n",
      "[39/100] train_loss=0.0911 | val_loss=0.0710 val_rmse=0.2663 val_r2=-8.2951val_corr=0.8453\n",
      "[40/100] train_loss=0.0888 | val_loss=0.0688 val_rmse=0.2622 val_r2=-10.4582val_corr=0.8471\n",
      "[41/100] train_loss=0.0877 | val_loss=0.0680 val_rmse=0.2606 val_r2=-11.5833val_corr=0.8464\n",
      "[42/100] train_loss=0.0873 | val_loss=0.0711 val_rmse=0.2665 val_r2=-11.3758val_corr=0.8411\n",
      "[43/100] train_loss=0.0895 | val_loss=0.1122 val_rmse=0.3349 val_r2=-12.6335val_corr=0.7318\n",
      "[44/100] train_loss=0.1144 | val_loss=0.0832 val_rmse=0.2883 val_r2=-18.1384val_corr=0.8396\n",
      "[45/100] train_loss=0.1006 | val_loss=0.0788 val_rmse=0.2805 val_r2=-7.5246val_corr=0.8418\n",
      "[46/100] train_loss=0.0934 | val_loss=0.0712 val_rmse=0.2668 val_r2=-5.1064val_corr=0.8493\n",
      "Early stopping at epoch 46 (best epoch=41, val_loss=0.0680)\n",
      "1 回目の学習が終了しました。ベストモデルを保存: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed0\n",
      "[1/100] train_loss=0.9534 | val_loss=0.5530 val_rmse=0.7428 val_r2=-15.1234val_corr=0.7268\n",
      "[2/100] train_loss=0.5641 | val_loss=0.3565 val_rmse=0.5965 val_r2=-12.4409val_corr=0.7586\n",
      "[3/100] train_loss=0.4174 | val_loss=0.3000 val_rmse=0.5475 val_r2=-8.1164val_corr=0.7698\n",
      "[4/100] train_loss=0.3589 | val_loss=0.2683 val_rmse=0.5177 val_r2=-7.1806val_corr=0.7721\n",
      "[5/100] train_loss=0.3234 | val_loss=0.2425 val_rmse=0.4922 val_r2=-12.7773val_corr=0.7696\n",
      "[6/100] train_loss=0.2937 | val_loss=0.2098 val_rmse=0.4578 val_r2=-6.1081val_corr=0.7794\n",
      "[7/100] train_loss=0.2467 | val_loss=0.1786 val_rmse=0.4225 val_r2=-13.1797val_corr=0.7760\n",
      "[8/100] train_loss=0.2171 | val_loss=0.1609 val_rmse=0.4010 val_r2=-24.2191val_corr=0.7686\n",
      "[9/100] train_loss=0.2010 | val_loss=0.1489 val_rmse=0.3858 val_r2=-11.6506val_corr=0.7727\n",
      "[10/100] train_loss=0.1866 | val_loss=0.1396 val_rmse=0.3736 val_r2=-11.9646val_corr=0.7891\n",
      "[11/100] train_loss=0.1776 | val_loss=0.1374 val_rmse=0.3706 val_r2=-10.2086val_corr=0.7888\n",
      "[12/100] train_loss=0.1718 | val_loss=0.1280 val_rmse=0.3577 val_r2=-10.7429val_corr=0.7949\n",
      "[13/100] train_loss=0.1636 | val_loss=0.1231 val_rmse=0.3509 val_r2=-9.8394val_corr=0.8024\n",
      "[14/100] train_loss=0.1605 | val_loss=0.1239 val_rmse=0.3520 val_r2=-16.6750val_corr=0.7946\n",
      "[15/100] train_loss=0.1570 | val_loss=0.1205 val_rmse=0.3471 val_r2=-14.8853val_corr=0.8015\n",
      "[16/100] train_loss=0.1539 | val_loss=0.1145 val_rmse=0.3383 val_r2=-9.2050val_corr=0.8077\n",
      "[17/100] train_loss=0.1503 | val_loss=0.1139 val_rmse=0.3375 val_r2=-6.6399val_corr=0.8043\n",
      "[18/100] train_loss=0.1493 | val_loss=0.1171 val_rmse=0.3422 val_r2=-10.2746val_corr=0.8010\n",
      "[19/100] train_loss=0.1467 | val_loss=0.1193 val_rmse=0.3454 val_r2=-8.2430val_corr=0.7974\n",
      "[20/100] train_loss=0.1459 | val_loss=0.1120 val_rmse=0.3346 val_r2=-7.6207val_corr=0.8051\n",
      "[21/100] train_loss=0.1446 | val_loss=0.1097 val_rmse=0.3312 val_r2=-6.2371val_corr=0.8100\n",
      "[22/100] train_loss=0.1385 | val_loss=0.1079 val_rmse=0.3284 val_r2=-5.6535val_corr=0.8122\n",
      "[23/100] train_loss=0.1371 | val_loss=0.1043 val_rmse=0.3229 val_r2=-6.7754val_corr=0.8107\n",
      "[24/100] train_loss=0.1342 | val_loss=0.1006 val_rmse=0.3171 val_r2=-6.2609val_corr=0.8155\n",
      "[25/100] train_loss=0.1301 | val_loss=0.1005 val_rmse=0.3170 val_r2=-8.2380val_corr=0.8133\n",
      "[26/100] train_loss=0.1283 | val_loss=0.0973 val_rmse=0.3119 val_r2=-8.6006val_corr=0.8171\n",
      "[27/100] train_loss=0.1262 | val_loss=0.0967 val_rmse=0.3110 val_r2=-8.0064val_corr=0.8162\n",
      "[28/100] train_loss=0.1248 | val_loss=0.0947 val_rmse=0.3077 val_r2=-7.1758val_corr=0.8199\n",
      "[29/100] train_loss=0.1230 | val_loss=0.0954 val_rmse=0.3089 val_r2=-8.4584val_corr=0.8178\n",
      "[30/100] train_loss=0.1250 | val_loss=0.0909 val_rmse=0.3015 val_r2=-8.4610val_corr=0.8202\n",
      "[31/100] train_loss=0.1204 | val_loss=0.0918 val_rmse=0.3030 val_r2=-8.4564val_corr=0.8175\n",
      "[32/100] train_loss=0.1188 | val_loss=0.0905 val_rmse=0.3008 val_r2=-11.5755val_corr=0.8189\n",
      "[33/100] train_loss=0.1144 | val_loss=0.0861 val_rmse=0.2934 val_r2=-10.3718val_corr=0.8170\n",
      "[34/100] train_loss=0.1143 | val_loss=0.0864 val_rmse=0.2939 val_r2=-11.3253val_corr=0.8124\n",
      "[35/100] train_loss=0.1126 | val_loss=0.0838 val_rmse=0.2895 val_r2=-8.6258val_corr=0.8218\n",
      "[36/100] train_loss=0.1107 | val_loss=0.0822 val_rmse=0.2867 val_r2=-8.9205val_corr=0.8247\n",
      "[37/100] train_loss=0.1093 | val_loss=0.0796 val_rmse=0.2821 val_r2=-8.0735val_corr=0.8263\n",
      "[38/100] train_loss=0.1063 | val_loss=0.0784 val_rmse=0.2800 val_r2=-7.1312val_corr=0.8236\n",
      "[39/100] train_loss=0.1055 | val_loss=0.0773 val_rmse=0.2780 val_r2=-7.4519val_corr=0.8280\n",
      "[40/100] train_loss=0.1034 | val_loss=0.0773 val_rmse=0.2779 val_r2=-6.5918val_corr=0.8291\n",
      "[41/100] train_loss=0.1024 | val_loss=0.0761 val_rmse=0.2758 val_r2=-4.0718val_corr=0.8269\n",
      "[42/100] train_loss=0.1008 | val_loss=0.0719 val_rmse=0.2681 val_r2=-7.2350val_corr=0.8225\n",
      "[43/100] train_loss=0.0996 | val_loss=0.0726 val_rmse=0.2694 val_r2=-5.5214val_corr=0.8246\n",
      "[44/100] train_loss=0.0973 | val_loss=0.0709 val_rmse=0.2663 val_r2=-9.4589val_corr=0.8241\n",
      "[45/100] train_loss=0.0957 | val_loss=0.0700 val_rmse=0.2645 val_r2=-4.8451val_corr=0.8311\n",
      "[46/100] train_loss=0.0935 | val_loss=0.0654 val_rmse=0.2558 val_r2=-3.8825val_corr=0.8321\n",
      "[47/100] train_loss=0.0912 | val_loss=0.0626 val_rmse=0.2502 val_r2=-6.5416val_corr=0.8304\n",
      "[48/100] train_loss=0.0879 | val_loss=0.0586 val_rmse=0.2421 val_r2=-3.4236val_corr=0.8402\n",
      "[49/100] train_loss=0.0845 | val_loss=0.0561 val_rmse=0.2368 val_r2=-4.9595val_corr=0.8442\n",
      "[50/100] train_loss=0.0858 | val_loss=0.0563 val_rmse=0.2372 val_r2=-4.1640val_corr=0.8443\n",
      "[51/100] train_loss=0.0821 | val_loss=0.0561 val_rmse=0.2369 val_r2=-4.7193val_corr=0.8467\n",
      "[52/100] train_loss=0.0810 | val_loss=0.0515 val_rmse=0.2268 val_r2=-3.8579val_corr=0.8564\n",
      "[53/100] train_loss=0.0787 | val_loss=0.0517 val_rmse=0.2273 val_r2=-3.3657val_corr=0.8570\n",
      "[54/100] train_loss=0.0766 | val_loss=0.0481 val_rmse=0.2192 val_r2=-3.1219val_corr=0.8600\n",
      "[55/100] train_loss=0.0734 | val_loss=0.0467 val_rmse=0.2160 val_r2=-3.4185val_corr=0.8641\n",
      "[56/100] train_loss=0.0704 | val_loss=0.0449 val_rmse=0.2118 val_r2=-5.4293val_corr=0.8660\n",
      "[57/100] train_loss=0.0687 | val_loss=0.0423 val_rmse=0.2056 val_r2=-2.7239val_corr=0.8686\n",
      "[58/100] train_loss=0.0670 | val_loss=0.0441 val_rmse=0.2100 val_r2=-3.3149val_corr=0.8684\n",
      "[59/100] train_loss=0.0671 | val_loss=0.0403 val_rmse=0.2007 val_r2=-2.7735val_corr=0.8726\n",
      "[60/100] train_loss=0.0650 | val_loss=0.0403 val_rmse=0.2008 val_r2=-3.5708val_corr=0.8731\n",
      "[61/100] train_loss=0.0633 | val_loss=0.0403 val_rmse=0.2006 val_r2=-2.5066val_corr=0.8670\n",
      "[62/100] train_loss=0.0704 | val_loss=0.0520 val_rmse=0.2281 val_r2=-1.8989val_corr=0.8610\n",
      "[63/100] train_loss=0.0741 | val_loss=0.0499 val_rmse=0.2234 val_r2=-5.0305val_corr=0.8563\n",
      "[64/100] train_loss=0.0693 | val_loss=0.0408 val_rmse=0.2019 val_r2=-2.6928val_corr=0.8712\n",
      "Early stopping at epoch 64 (best epoch=59, val_loss=0.0403)\n",
      "2 回目の学習が終了しました。ベストモデルを保存: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed1\n",
      "[1/100] train_loss=1.1950 | val_loss=0.6490 val_rmse=0.8056 val_r2=-43.4398val_corr=0.6814\n",
      "[2/100] train_loss=0.6667 | val_loss=0.4443 val_rmse=0.6665 val_r2=-24.8097val_corr=0.6850\n",
      "[3/100] train_loss=0.4692 | val_loss=0.3472 val_rmse=0.5892 val_r2=-14.5240val_corr=0.7630\n",
      "[4/100] train_loss=0.3933 | val_loss=0.3130 val_rmse=0.5594 val_r2=-11.5183val_corr=0.7760\n",
      "[5/100] train_loss=0.3622 | val_loss=0.2984 val_rmse=0.5463 val_r2=-13.9114val_corr=0.7666\n",
      "[6/100] train_loss=0.3432 | val_loss=0.2865 val_rmse=0.5352 val_r2=-10.7785val_corr=0.7657\n",
      "[7/100] train_loss=0.3242 | val_loss=0.2591 val_rmse=0.5090 val_r2=-9.1653val_corr=0.7777\n",
      "[8/100] train_loss=0.2818 | val_loss=0.2140 val_rmse=0.4626 val_r2=-11.1831val_corr=0.7801\n",
      "[9/100] train_loss=0.2406 | val_loss=0.1932 val_rmse=0.4395 val_r2=-3.8300val_corr=0.7835\n",
      "[10/100] train_loss=0.2243 | val_loss=0.1791 val_rmse=0.4232 val_r2=-9.7769val_corr=0.7796\n",
      "[11/100] train_loss=0.2095 | val_loss=0.1675 val_rmse=0.4092 val_r2=-7.7035val_corr=0.7870\n",
      "[12/100] train_loss=0.1988 | val_loss=0.1601 val_rmse=0.4002 val_r2=-5.2099val_corr=0.7959\n",
      "[13/100] train_loss=0.1864 | val_loss=0.1547 val_rmse=0.3933 val_r2=-6.9717val_corr=0.8026\n",
      "[14/100] train_loss=0.1773 | val_loss=0.1504 val_rmse=0.3878 val_r2=-3.9748val_corr=0.8120\n",
      "[15/100] train_loss=0.1717 | val_loss=0.1389 val_rmse=0.3727 val_r2=-3.9133val_corr=0.8189\n",
      "[16/100] train_loss=0.1628 | val_loss=0.1295 val_rmse=0.3599 val_r2=-3.7453val_corr=0.8225\n",
      "[17/100] train_loss=0.1567 | val_loss=0.1267 val_rmse=0.3559 val_r2=-2.8825val_corr=0.8160\n",
      "[18/100] train_loss=0.1563 | val_loss=0.1267 val_rmse=0.3559 val_r2=-2.9271val_corr=0.8192\n",
      "[19/100] train_loss=0.1520 | val_loss=0.1247 val_rmse=0.3532 val_r2=-3.3572val_corr=0.8167\n",
      "[20/100] train_loss=0.1475 | val_loss=0.1144 val_rmse=0.3383 val_r2=-4.0417val_corr=0.8247\n",
      "[21/100] train_loss=0.1420 | val_loss=0.1109 val_rmse=0.3330 val_r2=-2.2758val_corr=0.8220\n",
      "[22/100] train_loss=0.1350 | val_loss=0.1028 val_rmse=0.3206 val_r2=-2.2801val_corr=0.8273\n",
      "[23/100] train_loss=0.1306 | val_loss=0.1013 val_rmse=0.3183 val_r2=-2.6416val_corr=0.8283\n",
      "[24/100] train_loss=0.1303 | val_loss=0.1031 val_rmse=0.3211 val_r2=-3.2977val_corr=0.8135\n",
      "[25/100] train_loss=0.1315 | val_loss=0.0962 val_rmse=0.3102 val_r2=-4.0751val_corr=0.8247\n",
      "[26/100] train_loss=0.1290 | val_loss=0.1000 val_rmse=0.3163 val_r2=-4.4760val_corr=0.8308\n",
      "[27/100] train_loss=0.1249 | val_loss=0.0893 val_rmse=0.2989 val_r2=-3.3844val_corr=0.8379\n",
      "[28/100] train_loss=0.1192 | val_loss=0.0894 val_rmse=0.2991 val_r2=-3.7825val_corr=0.8349\n",
      "[29/100] train_loss=0.1146 | val_loss=0.0819 val_rmse=0.2862 val_r2=-3.6491val_corr=0.8378\n",
      "[30/100] train_loss=0.1088 | val_loss=0.0769 val_rmse=0.2773 val_r2=-4.7370val_corr=0.8431\n",
      "[31/100] train_loss=0.1035 | val_loss=0.0737 val_rmse=0.2714 val_r2=-3.6082val_corr=0.8430\n",
      "[32/100] train_loss=0.1009 | val_loss=0.0716 val_rmse=0.2675 val_r2=-2.8814val_corr=0.8466\n",
      "[33/100] train_loss=0.0990 | val_loss=0.0727 val_rmse=0.2695 val_r2=-2.1687val_corr=0.8477\n",
      "[34/100] train_loss=0.0990 | val_loss=0.0713 val_rmse=0.2670 val_r2=-1.9847val_corr=0.8484\n",
      "[35/100] train_loss=0.0956 | val_loss=0.0657 val_rmse=0.2562 val_r2=-2.4761val_corr=0.8492\n",
      "[36/100] train_loss=0.0948 | val_loss=0.0652 val_rmse=0.2553 val_r2=-2.9563val_corr=0.8508\n",
      "[37/100] train_loss=0.0910 | val_loss=0.0630 val_rmse=0.2510 val_r2=-1.7661val_corr=0.8529\n",
      "[38/100] train_loss=0.0897 | val_loss=0.0620 val_rmse=0.2491 val_r2=-1.3186val_corr=0.8535\n",
      "[39/100] train_loss=0.0887 | val_loss=0.0609 val_rmse=0.2468 val_r2=-1.1110val_corr=0.8547\n",
      "[40/100] train_loss=0.0871 | val_loss=0.0601 val_rmse=0.2451 val_r2=-1.4135val_corr=0.8565\n",
      "[41/100] train_loss=0.0856 | val_loss=0.0596 val_rmse=0.2441 val_r2=-1.0833val_corr=0.8568\n",
      "[42/100] train_loss=0.0848 | val_loss=0.0577 val_rmse=0.2401 val_r2=-1.2616val_corr=0.8606\n",
      "[43/100] train_loss=0.0840 | val_loss=0.0579 val_rmse=0.2406 val_r2=-1.5795val_corr=0.8586\n",
      "[44/100] train_loss=0.0844 | val_loss=0.0570 val_rmse=0.2387 val_r2=-1.5105val_corr=0.8608\n",
      "[45/100] train_loss=0.0829 | val_loss=0.0561 val_rmse=0.2368 val_r2=-1.5586val_corr=0.8612\n",
      "[46/100] train_loss=0.0813 | val_loss=0.0546 val_rmse=0.2337 val_r2=-1.1021val_corr=0.8648\n",
      "[47/100] train_loss=0.0808 | val_loss=0.0543 val_rmse=0.2330 val_r2=-1.4448val_corr=0.8657\n",
      "[48/100] train_loss=0.0806 | val_loss=0.0557 val_rmse=0.2359 val_r2=-0.9994val_corr=0.8655\n",
      "[49/100] train_loss=0.0801 | val_loss=0.0540 val_rmse=0.2323 val_r2=-2.0477val_corr=0.8639\n",
      "[50/100] train_loss=0.0806 | val_loss=0.0552 val_rmse=0.2349 val_r2=-1.9843val_corr=0.8632\n",
      "[51/100] train_loss=0.0785 | val_loss=0.0523 val_rmse=0.2286 val_r2=-1.4183val_corr=0.8669\n",
      "[52/100] train_loss=0.0768 | val_loss=0.0508 val_rmse=0.2254 val_r2=-1.1052val_corr=0.8689\n",
      "[53/100] train_loss=0.0758 | val_loss=0.0498 val_rmse=0.2231 val_r2=-1.2788val_corr=0.8686\n",
      "[54/100] train_loss=0.0746 | val_loss=0.0486 val_rmse=0.2204 val_r2=-0.8695val_corr=0.8707\n",
      "[55/100] train_loss=0.0739 | val_loss=0.0484 val_rmse=0.2200 val_r2=-0.9707val_corr=0.8702\n",
      "[56/100] train_loss=0.0747 | val_loss=0.0487 val_rmse=0.2207 val_r2=-0.9916val_corr=0.8730\n",
      "[57/100] train_loss=0.0731 | val_loss=0.0481 val_rmse=0.2193 val_r2=-0.9010val_corr=0.8712\n",
      "[58/100] train_loss=0.0719 | val_loss=0.0476 val_rmse=0.2182 val_r2=-0.8247val_corr=0.8774\n",
      "[59/100] train_loss=0.0737 | val_loss=0.0491 val_rmse=0.2214 val_r2=-0.5978val_corr=0.8741\n",
      "[60/100] train_loss=0.0729 | val_loss=0.0462 val_rmse=0.2150 val_r2=-0.3356val_corr=0.8789\n",
      "[61/100] train_loss=0.0705 | val_loss=0.0452 val_rmse=0.2126 val_r2=-0.2399val_corr=0.8770\n",
      "[62/100] train_loss=0.0691 | val_loss=0.0431 val_rmse=0.2074 val_r2=-0.3691val_corr=0.8775\n",
      "[63/100] train_loss=0.0683 | val_loss=0.0432 val_rmse=0.2078 val_r2=-0.4823val_corr=0.8775\n",
      "[64/100] train_loss=0.0690 | val_loss=0.0430 val_rmse=0.2073 val_r2=-0.3549val_corr=0.8788\n",
      "[65/100] train_loss=0.0672 | val_loss=0.0408 val_rmse=0.2020 val_r2=-0.4809val_corr=0.8796\n",
      "[66/100] train_loss=0.0671 | val_loss=0.0397 val_rmse=0.1992 val_r2=-0.3052val_corr=0.8810\n",
      "[67/100] train_loss=0.0650 | val_loss=0.0403 val_rmse=0.2006 val_r2=-0.6687val_corr=0.8777\n",
      "[68/100] train_loss=0.0646 | val_loss=0.0385 val_rmse=0.1960 val_r2=-0.4294val_corr=0.8825\n",
      "[69/100] train_loss=0.0645 | val_loss=0.0376 val_rmse=0.1937 val_r2=-0.4778val_corr=0.8832\n",
      "[70/100] train_loss=0.0636 | val_loss=0.0376 val_rmse=0.1938 val_r2=-1.0812val_corr=0.8848\n",
      "[71/100] train_loss=0.0623 | val_loss=0.0370 val_rmse=0.1922 val_r2=-0.4109val_corr=0.8869\n",
      "[72/100] train_loss=0.0622 | val_loss=0.0382 val_rmse=0.1954 val_r2=-0.8143val_corr=0.8889\n",
      "[73/100] train_loss=0.0630 | val_loss=0.0361 val_rmse=0.1898 val_r2=-0.9041val_corr=0.8895\n",
      "[74/100] train_loss=0.0617 | val_loss=0.0357 val_rmse=0.1888 val_r2=-0.1248val_corr=0.8907\n",
      "[75/100] train_loss=0.0606 | val_loss=0.0356 val_rmse=0.1884 val_r2=-0.3723val_corr=0.8907\n",
      "[76/100] train_loss=0.0597 | val_loss=0.0343 val_rmse=0.1852 val_r2=0.0460val_corr=0.8939\n",
      "[77/100] train_loss=0.0600 | val_loss=0.0336 val_rmse=0.1831 val_r2=0.0486val_corr=0.8959\n",
      "[78/100] train_loss=0.0593 | val_loss=0.0333 val_rmse=0.1824 val_r2=-0.4106val_corr=0.8985\n",
      "[79/100] train_loss=0.0580 | val_loss=0.0333 val_rmse=0.1822 val_r2=0.1077val_corr=0.9010\n",
      "[80/100] train_loss=0.0589 | val_loss=0.0344 val_rmse=0.1854 val_r2=-1.0085val_corr=0.8998\n",
      "[81/100] train_loss=0.0584 | val_loss=0.0323 val_rmse=0.1795 val_r2=-0.0984val_corr=0.9011\n",
      "[82/100] train_loss=0.0573 | val_loss=0.0311 val_rmse=0.1763 val_r2=0.1880val_corr=0.9029\n",
      "[83/100] train_loss=0.0566 | val_loss=0.0310 val_rmse=0.1759 val_r2=0.2476val_corr=0.9050\n",
      "[84/100] train_loss=0.0559 | val_loss=0.0300 val_rmse=0.1732 val_r2=-0.0877val_corr=0.9088\n",
      "[85/100] train_loss=0.0560 | val_loss=0.0297 val_rmse=0.1723 val_r2=0.2902val_corr=0.9103\n",
      "[86/100] train_loss=0.0543 | val_loss=0.0293 val_rmse=0.1712 val_r2=-0.0539val_corr=0.9136\n",
      "[87/100] train_loss=0.0545 | val_loss=0.0291 val_rmse=0.1705 val_r2=0.2986val_corr=0.9129\n",
      "[88/100] train_loss=0.0538 | val_loss=0.0287 val_rmse=0.1695 val_r2=0.2069val_corr=0.9143\n",
      "[89/100] train_loss=0.0532 | val_loss=0.0281 val_rmse=0.1675 val_r2=0.2190val_corr=0.9159\n",
      "[90/100] train_loss=0.0533 | val_loss=0.0281 val_rmse=0.1675 val_r2=0.3112val_corr=0.9187\n",
      "[91/100] train_loss=0.0530 | val_loss=0.0279 val_rmse=0.1669 val_r2=0.2021val_corr=0.9200\n",
      "[92/100] train_loss=0.0523 | val_loss=0.0269 val_rmse=0.1638 val_r2=0.2030val_corr=0.9230\n",
      "[93/100] train_loss=0.0524 | val_loss=0.0273 val_rmse=0.1650 val_r2=-0.0594val_corr=0.9207\n",
      "[94/100] train_loss=0.0521 | val_loss=0.0272 val_rmse=0.1648 val_r2=-0.3286val_corr=0.9232\n",
      "[95/100] train_loss=0.0521 | val_loss=0.0265 val_rmse=0.1626 val_r2=0.1923val_corr=0.9218\n",
      "[96/100] train_loss=0.0521 | val_loss=0.0271 val_rmse=0.1645 val_r2=0.0398val_corr=0.9263\n",
      "[97/100] train_loss=0.0513 | val_loss=0.0268 val_rmse=0.1635 val_r2=-0.4377val_corr=0.9221\n",
      "[98/100] train_loss=0.0514 | val_loss=0.0272 val_rmse=0.1648 val_r2=-0.8520val_corr=0.9209\n",
      "[99/100] train_loss=0.0515 | val_loss=0.0266 val_rmse=0.1630 val_r2=-1.1147val_corr=0.9253\n",
      "[100/100] train_loss=0.0519 | val_loss=0.0313 val_rmse=0.1768 val_r2=0.2867val_corr=0.9282\n",
      "Early stopping at epoch 100 (best epoch=95, val_loss=0.0265)\n",
      "3 回目の学習が終了しました。ベストモデルを保存: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed2\n",
      "[1/100] train_loss=0.8523 | val_loss=0.6662 val_rmse=0.8156 val_r2=-10.4593val_corr=0.7371\n",
      "[2/100] train_loss=0.5238 | val_loss=0.4099 val_rmse=0.6397 val_r2=-4.2252val_corr=0.7838\n",
      "[3/100] train_loss=0.3679 | val_loss=0.3324 val_rmse=0.5761 val_r2=-6.1816val_corr=0.7975\n",
      "[4/100] train_loss=0.3314 | val_loss=0.3309 val_rmse=0.5751 val_r2=-9.6734val_corr=0.7862\n",
      "[5/100] train_loss=0.3064 | val_loss=0.2783 val_rmse=0.5273 val_r2=-5.9665val_corr=0.7982\n",
      "[6/100] train_loss=0.2576 | val_loss=0.2236 val_rmse=0.4728 val_r2=-6.3922val_corr=0.7998\n",
      "[7/100] train_loss=0.2256 | val_loss=0.2059 val_rmse=0.4538 val_r2=-8.3188val_corr=0.8084\n",
      "[8/100] train_loss=0.2092 | val_loss=0.2009 val_rmse=0.4482 val_r2=-5.8946val_corr=0.8144\n",
      "[9/100] train_loss=0.1985 | val_loss=0.1844 val_rmse=0.4294 val_r2=-14.8994val_corr=0.8148\n",
      "[10/100] train_loss=0.1832 | val_loss=0.1680 val_rmse=0.4098 val_r2=-6.0865val_corr=0.8243\n",
      "[11/100] train_loss=0.1724 | val_loss=0.1567 val_rmse=0.3959 val_r2=-7.1847val_corr=0.8187\n",
      "[12/100] train_loss=0.1662 | val_loss=0.1472 val_rmse=0.3835 val_r2=-5.0920val_corr=0.8302\n",
      "[13/100] train_loss=0.1560 | val_loss=0.1378 val_rmse=0.3711 val_r2=-4.7894val_corr=0.8317\n",
      "[14/100] train_loss=0.1460 | val_loss=0.1272 val_rmse=0.3566 val_r2=-4.4149val_corr=0.8343\n",
      "[15/100] train_loss=0.1377 | val_loss=0.1201 val_rmse=0.3465 val_r2=-4.4639val_corr=0.8388\n",
      "[16/100] train_loss=0.1371 | val_loss=0.1213 val_rmse=0.3481 val_r2=-4.5415val_corr=0.8406\n",
      "[17/100] train_loss=0.1333 | val_loss=0.1172 val_rmse=0.3423 val_r2=-3.4470val_corr=0.8402\n",
      "[18/100] train_loss=0.1288 | val_loss=0.1113 val_rmse=0.3336 val_r2=-4.2458val_corr=0.8427\n",
      "[19/100] train_loss=0.1244 | val_loss=0.1091 val_rmse=0.3302 val_r2=-2.7592val_corr=0.8479\n",
      "[20/100] train_loss=0.1227 | val_loss=0.1043 val_rmse=0.3229 val_r2=-4.2463val_corr=0.8463\n",
      "[21/100] train_loss=0.1196 | val_loss=0.1068 val_rmse=0.3262 val_r2=-3.4205val_corr=0.8485\n",
      "[22/100] train_loss=0.1173 | val_loss=0.0999 val_rmse=0.3160 val_r2=-2.9383val_corr=0.8523\n",
      "[23/100] train_loss=0.1128 | val_loss=0.0948 val_rmse=0.3079 val_r2=-2.6865val_corr=0.8499\n",
      "[24/100] train_loss=0.1112 | val_loss=0.0942 val_rmse=0.3068 val_r2=-1.9628val_corr=0.8504\n",
      "[25/100] train_loss=0.1075 | val_loss=0.0901 val_rmse=0.3001 val_r2=-1.4567val_corr=0.8506\n",
      "[26/100] train_loss=0.1048 | val_loss=0.0878 val_rmse=0.2964 val_r2=-1.6155val_corr=0.8512\n",
      "[27/100] train_loss=0.1035 | val_loss=0.0880 val_rmse=0.2966 val_r2=-1.9257val_corr=0.8527\n",
      "[28/100] train_loss=0.1048 | val_loss=0.0872 val_rmse=0.2952 val_r2=-1.8814val_corr=0.8546\n",
      "[29/100] train_loss=0.1016 | val_loss=0.0837 val_rmse=0.2893 val_r2=-2.2515val_corr=0.8561\n",
      "[30/100] train_loss=0.0987 | val_loss=0.0830 val_rmse=0.2881 val_r2=-1.1299val_corr=0.8566\n",
      "[31/100] train_loss=0.0971 | val_loss=0.0791 val_rmse=0.2812 val_r2=-0.8757val_corr=0.8604\n",
      "[32/100] train_loss=0.0973 | val_loss=0.0803 val_rmse=0.2834 val_r2=-2.8219val_corr=0.8588\n",
      "[33/100] train_loss=0.0984 | val_loss=0.0777 val_rmse=0.2787 val_r2=-1.9730val_corr=0.8568\n",
      "[34/100] train_loss=0.0959 | val_loss=0.0786 val_rmse=0.2804 val_r2=-0.9792val_corr=0.8605\n",
      "[35/100] train_loss=0.0937 | val_loss=0.0748 val_rmse=0.2735 val_r2=-1.7710val_corr=0.8611\n",
      "[36/100] train_loss=0.0917 | val_loss=0.0728 val_rmse=0.2698 val_r2=-1.0056val_corr=0.8652\n",
      "[37/100] train_loss=0.0891 | val_loss=0.0727 val_rmse=0.2697 val_r2=-1.4197val_corr=0.8622\n",
      "[38/100] train_loss=0.0888 | val_loss=0.0783 val_rmse=0.2797 val_r2=-0.9935val_corr=0.8549\n",
      "[39/100] train_loss=0.0926 | val_loss=0.0711 val_rmse=0.2666 val_r2=-4.7221val_corr=0.8564\n",
      "[40/100] train_loss=0.0879 | val_loss=0.0659 val_rmse=0.2567 val_r2=-1.6662val_corr=0.8617\n",
      "[41/100] train_loss=0.0846 | val_loss=0.0647 val_rmse=0.2544 val_r2=-0.7251val_corr=0.8667\n",
      "[42/100] train_loss=0.0805 | val_loss=0.0592 val_rmse=0.2433 val_r2=-0.8423val_corr=0.8654\n",
      "[43/100] train_loss=0.0776 | val_loss=0.0586 val_rmse=0.2422 val_r2=-0.4647val_corr=0.8681\n",
      "[44/100] train_loss=0.0748 | val_loss=0.0558 val_rmse=0.2362 val_r2=-0.5649val_corr=0.8699\n",
      "[45/100] train_loss=0.0740 | val_loss=0.0536 val_rmse=0.2316 val_r2=-1.1241val_corr=0.8719\n",
      "[46/100] train_loss=0.0724 | val_loss=0.0525 val_rmse=0.2292 val_r2=-0.6006val_corr=0.8736\n",
      "[47/100] train_loss=0.0708 | val_loss=0.0497 val_rmse=0.2230 val_r2=-0.4619val_corr=0.8750\n",
      "[48/100] train_loss=0.0698 | val_loss=0.0488 val_rmse=0.2209 val_r2=-0.3125val_corr=0.8741\n",
      "[49/100] train_loss=0.0686 | val_loss=0.0481 val_rmse=0.2192 val_r2=-0.2406val_corr=0.8746\n",
      "[50/100] train_loss=0.0683 | val_loss=0.0486 val_rmse=0.2205 val_r2=-0.0866val_corr=0.8754\n",
      "[51/100] train_loss=0.0682 | val_loss=0.0488 val_rmse=0.2209 val_r2=0.0369val_corr=0.8755\n",
      "[52/100] train_loss=0.0678 | val_loss=0.0473 val_rmse=0.2175 val_r2=-2.0585val_corr=0.8757\n",
      "[53/100] train_loss=0.0671 | val_loss=0.0467 val_rmse=0.2161 val_r2=-0.9900val_corr=0.8776\n",
      "[54/100] train_loss=0.0662 | val_loss=0.0456 val_rmse=0.2134 val_r2=0.0558val_corr=0.8816\n",
      "[55/100] train_loss=0.0641 | val_loss=0.0436 val_rmse=0.2089 val_r2=-0.3374val_corr=0.8817\n",
      "[56/100] train_loss=0.0643 | val_loss=0.0446 val_rmse=0.2112 val_r2=-0.9128val_corr=0.8842\n",
      "[57/100] train_loss=0.0637 | val_loss=0.0436 val_rmse=0.2088 val_r2=-0.9858val_corr=0.8825\n",
      "[58/100] train_loss=0.0630 | val_loss=0.0428 val_rmse=0.2069 val_r2=-0.3117val_corr=0.8850\n",
      "[59/100] train_loss=0.0623 | val_loss=0.0438 val_rmse=0.2093 val_r2=-0.0322val_corr=0.8868\n",
      "[60/100] train_loss=0.0617 | val_loss=0.0426 val_rmse=0.2063 val_r2=-0.2904val_corr=0.8840\n",
      "[61/100] train_loss=0.0613 | val_loss=0.0405 val_rmse=0.2012 val_r2=-0.7566val_corr=0.8853\n",
      "[62/100] train_loss=0.0604 | val_loss=0.0410 val_rmse=0.2026 val_r2=-0.2790val_corr=0.8875\n",
      "[63/100] train_loss=0.0612 | val_loss=0.0398 val_rmse=0.1994 val_r2=-1.5517val_corr=0.8909\n",
      "[64/100] train_loss=0.0597 | val_loss=0.0406 val_rmse=0.2015 val_r2=-0.3061val_corr=0.8906\n",
      "[65/100] train_loss=0.0608 | val_loss=0.0404 val_rmse=0.2010 val_r2=-1.1842val_corr=0.8945\n",
      "[66/100] train_loss=0.0608 | val_loss=0.0389 val_rmse=0.1973 val_r2=-3.1393val_corr=0.8930\n",
      "[67/100] train_loss=0.0597 | val_loss=0.0416 val_rmse=0.2039 val_r2=-0.4491val_corr=0.8986\n",
      "[68/100] train_loss=0.0594 | val_loss=0.0391 val_rmse=0.1978 val_r2=-0.6243val_corr=0.8928\n",
      "[69/100] train_loss=0.0579 | val_loss=0.0374 val_rmse=0.1933 val_r2=-1.6723val_corr=0.8992\n",
      "[70/100] train_loss=0.0583 | val_loss=0.0365 val_rmse=0.1910 val_r2=-1.4052val_corr=0.9029\n",
      "[71/100] train_loss=0.0586 | val_loss=0.0369 val_rmse=0.1921 val_r2=-2.0851val_corr=0.9030\n",
      "[72/100] train_loss=0.0563 | val_loss=0.0374 val_rmse=0.1934 val_r2=-2.2008val_corr=0.9057\n",
      "[73/100] train_loss=0.0564 | val_loss=0.0348 val_rmse=0.1865 val_r2=-0.7285val_corr=0.9070\n",
      "[74/100] train_loss=0.0547 | val_loss=0.0341 val_rmse=0.1846 val_r2=-0.5005val_corr=0.9097\n",
      "[75/100] train_loss=0.0540 | val_loss=0.0332 val_rmse=0.1822 val_r2=-0.2790val_corr=0.9141\n",
      "[76/100] train_loss=0.0544 | val_loss=0.0338 val_rmse=0.1839 val_r2=-1.1206val_corr=0.9140\n",
      "[77/100] train_loss=0.0539 | val_loss=0.0339 val_rmse=0.1841 val_r2=-0.1538val_corr=0.9142\n",
      "[78/100] train_loss=0.0543 | val_loss=0.0336 val_rmse=0.1834 val_r2=-1.6231val_corr=0.9158\n",
      "[79/100] train_loss=0.0533 | val_loss=0.0326 val_rmse=0.1806 val_r2=-1.0645val_corr=0.9146\n",
      "[80/100] train_loss=0.0528 | val_loss=0.0314 val_rmse=0.1773 val_r2=-0.2880val_corr=0.9193\n",
      "[81/100] train_loss=0.0520 | val_loss=0.0320 val_rmse=0.1789 val_r2=-0.9678val_corr=0.9179\n",
      "[82/100] train_loss=0.0516 | val_loss=0.0328 val_rmse=0.1811 val_r2=-0.9489val_corr=0.9121\n",
      "[83/100] train_loss=0.0534 | val_loss=0.0321 val_rmse=0.1793 val_r2=-1.2868val_corr=0.9188\n",
      "[84/100] train_loss=0.0522 | val_loss=0.0324 val_rmse=0.1799 val_r2=-0.6294val_corr=0.9206\n",
      "[85/100] train_loss=0.0515 | val_loss=0.0318 val_rmse=0.1784 val_r2=-0.1287val_corr=0.9200\n",
      "Early stopping at epoch 85 (best epoch=80, val_loss=0.0314)\n",
      "4 回目の学習が終了しました。ベストモデルを保存: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed3\n",
      "[1/100] train_loss=0.8933 | val_loss=0.6525 val_rmse=0.8068 val_r2=-35.7306val_corr=0.7119\n",
      "[2/100] train_loss=0.5427 | val_loss=0.4185 val_rmse=0.6459 val_r2=-25.3725val_corr=0.7694\n",
      "[3/100] train_loss=0.4016 | val_loss=0.3459 val_rmse=0.5871 val_r2=-4.7897val_corr=0.7972\n",
      "[4/100] train_loss=0.3461 | val_loss=0.3030 val_rmse=0.5498 val_r2=-13.2363val_corr=0.7931\n",
      "[5/100] train_loss=0.3125 | val_loss=0.2687 val_rmse=0.5177 val_r2=-9.8894val_corr=0.8073\n",
      "[6/100] train_loss=0.2818 | val_loss=0.2234 val_rmse=0.4722 val_r2=-6.5124val_corr=0.8077\n",
      "[7/100] train_loss=0.2595 | val_loss=0.2318 val_rmse=0.4811 val_r2=-7.2827val_corr=0.8032\n",
      "[8/100] train_loss=0.2355 | val_loss=0.1929 val_rmse=0.4389 val_r2=-8.4140val_corr=0.8112\n",
      "[9/100] train_loss=0.2155 | val_loss=0.1785 val_rmse=0.4223 val_r2=-9.3518val_corr=0.8156\n",
      "[10/100] train_loss=0.2001 | val_loss=0.1690 val_rmse=0.4108 val_r2=-8.9496val_corr=0.8204\n",
      "[11/100] train_loss=0.1917 | val_loss=0.1682 val_rmse=0.4099 val_r2=-10.0314val_corr=0.8249\n",
      "[12/100] train_loss=0.1842 | val_loss=0.1598 val_rmse=0.3995 val_r2=-4.5379val_corr=0.8317\n",
      "[13/100] train_loss=0.1783 | val_loss=0.1507 val_rmse=0.3879 val_r2=-4.5958val_corr=0.8344\n",
      "[14/100] train_loss=0.1699 | val_loss=0.1418 val_rmse=0.3763 val_r2=-3.7443val_corr=0.8341\n",
      "[15/100] train_loss=0.1607 | val_loss=0.1342 val_rmse=0.3660 val_r2=-3.4997val_corr=0.8343\n",
      "[16/100] train_loss=0.1530 | val_loss=0.1268 val_rmse=0.3560 val_r2=-5.8455val_corr=0.8320\n",
      "[17/100] train_loss=0.1513 | val_loss=0.1282 val_rmse=0.3578 val_r2=-3.4899val_corr=0.8384\n",
      "[18/100] train_loss=0.1431 | val_loss=0.1155 val_rmse=0.3398 val_r2=-3.5374val_corr=0.8409\n",
      "[19/100] train_loss=0.1368 | val_loss=0.1125 val_rmse=0.3354 val_r2=-4.4049val_corr=0.8428\n",
      "[20/100] train_loss=0.1326 | val_loss=0.1109 val_rmse=0.3330 val_r2=-4.7089val_corr=0.8445\n",
      "[21/100] train_loss=0.1292 | val_loss=0.1064 val_rmse=0.3262 val_r2=-2.9985val_corr=0.8484\n",
      "[22/100] train_loss=0.1268 | val_loss=0.1070 val_rmse=0.3270 val_r2=-2.4535val_corr=0.8447\n",
      "[23/100] train_loss=0.1303 | val_loss=0.1032 val_rmse=0.3213 val_r2=-2.2868val_corr=0.8483\n",
      "[24/100] train_loss=0.1237 | val_loss=0.1006 val_rmse=0.3171 val_r2=-5.1842val_corr=0.8491\n",
      "[25/100] train_loss=0.1198 | val_loss=0.0976 val_rmse=0.3123 val_r2=-5.5373val_corr=0.8513\n",
      "[26/100] train_loss=0.1171 | val_loss=0.0945 val_rmse=0.3074 val_r2=-4.5605val_corr=0.8540\n",
      "[27/100] train_loss=0.1139 | val_loss=0.0930 val_rmse=0.3050 val_r2=-1.1063val_corr=0.8572\n",
      "[28/100] train_loss=0.1117 | val_loss=0.0906 val_rmse=0.3010 val_r2=-2.4491val_corr=0.8578\n",
      "[29/100] train_loss=0.1088 | val_loss=0.0880 val_rmse=0.2966 val_r2=-0.8992val_corr=0.8653\n",
      "[30/100] train_loss=0.1064 | val_loss=0.0841 val_rmse=0.2899 val_r2=-1.4405val_corr=0.8661\n",
      "[31/100] train_loss=0.1050 | val_loss=0.0827 val_rmse=0.2875 val_r2=-2.6810val_corr=0.8634\n",
      "[32/100] train_loss=0.1027 | val_loss=0.0832 val_rmse=0.2884 val_r2=-3.5408val_corr=0.8599\n",
      "[33/100] train_loss=0.1081 | val_loss=0.0861 val_rmse=0.2934 val_r2=-2.6517val_corr=0.8519\n",
      "[34/100] train_loss=0.1068 | val_loss=0.0877 val_rmse=0.2960 val_r2=-2.6897val_corr=0.8526\n",
      "[35/100] train_loss=0.1052 | val_loss=0.0801 val_rmse=0.2830 val_r2=-5.1696val_corr=0.8623\n",
      "[36/100] train_loss=0.1046 | val_loss=0.0828 val_rmse=0.2877 val_r2=-2.8923val_corr=0.8590\n",
      "[37/100] train_loss=0.1014 | val_loss=0.0790 val_rmse=0.2810 val_r2=-5.6407val_corr=0.8640\n",
      "[38/100] train_loss=0.0976 | val_loss=0.0746 val_rmse=0.2731 val_r2=-4.2724val_corr=0.8608\n",
      "[39/100] train_loss=0.0943 | val_loss=0.0705 val_rmse=0.2655 val_r2=-3.2375val_corr=0.8660\n",
      "[40/100] train_loss=0.0921 | val_loss=0.0695 val_rmse=0.2637 val_r2=-5.1970val_corr=0.8636\n",
      "[41/100] train_loss=0.0916 | val_loss=0.0721 val_rmse=0.2684 val_r2=-8.0032val_corr=0.8598\n",
      "[42/100] train_loss=0.0911 | val_loss=0.0664 val_rmse=0.2576 val_r2=-7.9995val_corr=0.8671\n",
      "[43/100] train_loss=0.0874 | val_loss=0.0671 val_rmse=0.2591 val_r2=-9.4968val_corr=0.8638\n",
      "[44/100] train_loss=0.0894 | val_loss=0.0743 val_rmse=0.2725 val_r2=-4.9877val_corr=0.8697\n",
      "[45/100] train_loss=0.0892 | val_loss=0.0712 val_rmse=0.2668 val_r2=-7.1458val_corr=0.8581\n",
      "[46/100] train_loss=0.0860 | val_loss=0.0668 val_rmse=0.2585 val_r2=-5.6074val_corr=0.8727\n",
      "[47/100] train_loss=0.0930 | val_loss=0.0681 val_rmse=0.2610 val_r2=-2.7215val_corr=0.8738\n",
      "Early stopping at epoch 47 (best epoch=42, val_loss=0.0664)\n",
      "5 回目の学習が終了しました。ベストモデルを保存: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed4\n",
      "すべての学習が終了しました。\n",
      "【全seedで最良】seed=2, best_epoch=95, val_loss=0.026455\n",
      "→ パス: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed2\n"
     ]
    }
   ],
   "source": [
    "seeds = [0, 1, 2, 3, 4]               # モデルの初期値を決める際のシード値\n",
    "y_pos = [Fe+Fd]\n",
    "global_best_val = float(\"inf\")\n",
    "\n",
    "rmse_list = np.zeros((5, 2))\n",
    "r2_list = np.zeros((5, 2))\n",
    "\n",
    "for s_id, seed in enumerate(seeds):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # DataLoaderの作成\n",
    "    random.shuffle(samples_train) # ランダムに並び替え\n",
    "    train_val_Dataset = FloodSeq2SeqDataset(samples_train)\n",
    "\n",
    "    n_total = len(train_val_Dataset)\n",
    "    n_train = int(n_total*0.85)\n",
    "    n_val = n_total - n_train\n",
    "    trainDataset, valDataset = random_split(train_val_Dataset, [n_train, n_val])\n",
    "\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=cfg[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "    valLoader = DataLoader(valDataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    # モデルの初期化\n",
    "    model = Seq2SeqLSTM(in_enc=cfg[\"in_enc\"], in_dec=cfg[\"in_dec\"], out_dim=cfg[\"out_dim\"],\n",
    "                        enc_hidden=cfg[\"enc_hidden\"], dec_hidden=cfg[\"dec_hidden\"],\n",
    "                        enc_layers=cfg[\"enc_layers\"], dec_layers=cfg[\"dec_layers\"],\n",
    "                        bridge_mode=cfg[\"bridge_mode\"], dropout=cfg[\"dropout\"],\n",
    "                        bidirectional_enc=cfg[\"bidirectional_enc\"],\n",
    "                        head_activation=cfg[\"head_activation\"],\n",
    "                        use_conv=cfg[\"use_conv\"]\n",
    "                       ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    best_opt_state = None\n",
    "    best_val_metrics = None\n",
    "\n",
    "    patience = cfg[\"patience\"]\n",
    "    min_delta = 1e-4\n",
    "\n",
    "    # 学習ループ（early stoppingあり）\n",
    "    for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "        # ---- 1. 学習 ----\n",
    "        train_loss = train_one_epoch(model, trainLoader, optimizer)\n",
    "\n",
    "        # ---- 2. 検証 ----\n",
    "        val_metrics = evaluate(model, valLoader)\n",
    "        val_loss = float(val_metrics[\"loss\"])\n",
    "\n",
    "        # ---- 3. ログ出力 ----\n",
    "        if epoch % cfg[\"print_every\"] == 0:\n",
    "            print(f\"[{epoch}/{cfg['epochs']}] \"\n",
    "                  f\"train_loss={train_loss:.4f} | \"\n",
    "                  f\"val_loss={val_loss:.4f} \"\n",
    "                  f\"val_rmse={val_metrics['rmse']:.4f} \"\n",
    "                  f\"val_r2={val_metrics['r2']:.4f}\"\n",
    "                  f\"val_corr={val_metrics['corr']:.4f}\")\n",
    "\n",
    "        # ---- 4. 改善チェック ----\n",
    "        if best_val - val_loss > min_delta:\n",
    "            best_val = val_loss\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            # ★ モデル重みをcloneして保持\n",
    "            best_model_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "            # ★ Optimizerの状態もcloneして保持（必要に応じて）\n",
    "            best_opt_state = {\n",
    "                \"state\": {\n",
    "                    k: {kk: (vv.detach().clone() if torch.is_tensor(vv) else vv)\n",
    "                        for kk, vv in v.items()}\n",
    "                    for k, v in optimizer.state_dict()[\"state\"].items()\n",
    "                },\n",
    "                \"param_groups\": [dict(g) for g in optimizer.state_dict()[\"param_groups\"]],\n",
    "            }\n",
    "    \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # ---- 5. Early Stopping 発動 ----\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} \"\n",
    "                  f\"(best epoch={best_epoch}, val_loss={best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "    assert best_model_state is not None, \"best_model_state が None です。学習が行われていない可能性があります。\"\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    # 学習終了後に、実スケールでの標準化パラメータを記録\n",
    "    metrics = evaluate_original_scale_by_index(model, valLoader, mean, std, data.columns, y_pos)\n",
    "    metrics_train = evaluate_original_scale_by_index(model, trainLoader, mean, std, data.columns, y_pos)\n",
    "    \n",
    "    rmse_list[s_id, 1] = metrics[\"rmse\"]\n",
    "    r2_list[s_id, 1] = metrics[\"r2\"]\n",
    "    rmse_list[s_id, 0] = metrics_train[\"rmse\"]\n",
    "    r2_list[s_id, 0] = metrics_train[\"r2\"]\n",
    "\n",
    "    ckpt_name = f\"seed{seed}\"\n",
    "    save_path_temp = os.path.join(save_dir, ckpt_name)\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": best_model_state,          # ★ベスト重み\n",
    "        \"optimizer_state\": best_opt_state,        # 再開したい場合に使用\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_loss\": best_val,\n",
    "        \"best_val_metrics\": best_val_metrics,     # スケール前の検証指標（任意）\n",
    "        \"val_metrics_original_scale\": metrics,    # 実スケール評価\n",
    "        \"cfg\": cfg,\n",
    "        \"seed\": seed,\n",
    "        \"scaler\": {\n",
    "            \"mean\": mean,                        # pandas.Series を含むなら必要に応じて .to_dict() でもOK\n",
    "            \"std\":  std,\n",
    "            \"data_columns\": list(data.columns),\n",
    "            \"y_pos\": y_pos,\n",
    "        }\n",
    "    }, save_path_temp)\n",
    "\n",
    "    print(f\"{s_id+1} 回目の学習が終了しました。ベストモデルを保存: {save_path_temp}\")\n",
    "\n",
    "    if best_val < global_best_val:\n",
    "        global_best_val = best_val\n",
    "        global_best_info = (seed, save_path_temp, best_epoch, best_val)\n",
    "\n",
    "\n",
    "print(\"すべての学習が終了しました。\")\n",
    "\n",
    "if global_best_info is not None:\n",
    "    g_seed, g_path, g_epoch, g_loss = global_best_info\n",
    "    print(f\"【全seedで最良】seed={g_seed}, best_epoch={g_epoch}, val_loss={g_loss:.6f}\")\n",
    "    print(f\"→ パス: {g_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d67165f-1ef2-4859-af01-4e44b45ed3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[234.60721622 243.36134737]\n",
      " [192.23022828 187.35264578]\n",
      " [155.46366138 151.8377075 ]\n",
      " [163.23028469 165.54694951]\n",
      " [236.37133092 240.46161806]]\n",
      "[[ -4.57521478 -11.58328152]\n",
      " [ -1.8313336   -2.77354991]\n",
      " [ -0.10170799   0.19234409]\n",
      " [ -0.47808661  -0.28804847]\n",
      " [ -6.01550584  -7.99945715]]\n",
      "結果をExcelファイルに保存しました。\n"
     ]
    }
   ],
   "source": [
    "# 結果をエクセルに保存\n",
    "print(rmse_list)\n",
    "print(r2_list)\n",
    "\n",
    "with pd.ExcelWriter(save_path_excel, engine=\"openpyxl\") as writer:\n",
    "    pd.DataFrame(rmse_list).to_excel(writer, sheet_name=\"rmse_list\", index=False, header=False)\n",
    "    pd.DataFrame(r2_list).to_excel(writer, sheet_name=\"r2_list\", index=False, header=False)\n",
    "\n",
    "print(\"結果をExcelファイルに保存しました。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45e28f-d575-4980-b328-70f1645fd386",
   "metadata": {},
   "source": [
    "## 最も良いモデルをロードし、テストデータで評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cefc937-e2a5-4304-8532-371309cb8711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryoya\\AppData\\Local\\Temp\\ipykernel_17284\\3204996539.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 最も良かったモデルのパスを指定\n",
    "best_model_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\seed2\"\n",
    "\n",
    "ckpt = torch.load(best_model_path, map_location=\"cpu\")\n",
    "\n",
    "# モデルの再構築\n",
    "cfg = ckpt[\"cfg\"]\n",
    "model = Seq2SeqLSTM(in_enc=cfg[\"in_enc\"], in_dec=cfg[\"in_dec\"], out_dim=cfg[\"out_dim\"],\n",
    "                    enc_hidden=cfg[\"enc_hidden\"], dec_hidden=cfg[\"dec_hidden\"],\n",
    "                    enc_layers=cfg[\"enc_layers\"], dec_layers=cfg[\"dec_layers\"],\n",
    "                    bridge_mode=cfg[\"bridge_mode\"], dropout=cfg[\"dropout\"],\n",
    "                    bidirectional_enc=cfg[\"bidirectional_enc\"],\n",
    "                    head_activation=cfg[\"head_activation\"],\n",
    "                    use_conv=cfg[\"use_conv\"]\n",
    "                   ).to(device)\n",
    "\n",
    "# パラメータをロード\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval() # 評価モードに切り替え\n",
    "\n",
    "# 標準化パラメータの取り出し\n",
    "scaler = ckpt[\"scaler\"]\n",
    "mean = scaler[\"mean\"]                # pandas.Series\n",
    "std  = scaler[\"std\"]                 # pandas.Series\n",
    "data_columns = scaler[\"data_columns\"]  # 列名リスト\n",
    "y_pos = scaler[\"y_pos\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91238f8a-d619-4836-9e3e-bcb76da11da0",
   "metadata": {},
   "source": [
    "## testLoaderを作成し、モデルの精度を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b8dae3c-db91-47eb-9d49-7c648593b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 465374.8566146736, 'rmse': 682.1838876832797, 'r2': -15.647573585423482, 'corr': 0.6554581143827124}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testDataset = FloodSeq2SeqDataset(samples_test)\n",
    "testLoader = DataLoader(testDataset, batch_size=cfg[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "metrics_test = evaluate_original_scale_by_index(model, testLoader, mean, std, data_columns, y_pos)\n",
    "\n",
    "print(metrics_test)\n",
    "\n",
    "test_save_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\CrossVal_5_result_test.xlsx\"\n",
    "\n",
    "# 辞書をDataFrameに変換\n",
    "df_metrics_test = pd.DataFrame([metrics_test])   # []で囲むと1行の表になる\n",
    "\n",
    "# Excelに出力\n",
    "df_metrics_test.to_excel(test_save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca029e-24fa-4ac8-b52a-d146aff5c80b",
   "metadata": {},
   "source": [
    "## 任意の時刻における予測をエクセルファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "793795fe-9679-4166-b854-e9b1759efe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excelに保存しました: C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\CrossVal_5_pred_2025_06_08.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 出力したい部分の開始行番号を指定（0はじまり）\n",
    "s = 56400\n",
    "pred_save_path = r\"C:\\Users\\ryoya\\MasterThesis\\MT_Furuie\\results\\Miwa_LSTM\\CrossVal_5\\model\\CrossVal_5_pred_2025_06_08.xlsx\"\n",
    "\n",
    "# エンコーダ開始時刻（予測期間は3日後から）\n",
    "# (4320: 2019_06_30), (38664: 2023_05_31), (38760: 2024_06_04), (13032: 2020_06_27), (13176: 2020_07_03), (48048: 2024_06_25), (56400: 2025_06_08)\n",
    "\n",
    "\n",
    "Fe, Fd, Fo = cfg[\"in_enc\"], cfg[\"in_dec\"], cfg[\"out_dim\"]\n",
    "\n",
    "enc_window = data_norm.iloc[s : s + Te]\n",
    "dec_window = data_norm.iloc[s + Te : s + Te + Td]\n",
    "# テンソル化\n",
    "enc_X = torch.tensor(enc_window.iloc[:, 0:Fe].to_numpy(dtype=np.float32))\n",
    "dec_X = torch.tensor(dec_window.iloc[:, Fe:Fe+Fd].to_numpy(dtype=np.float32))\n",
    "y     = torch.tensor(dec_window.iloc[:, Fe+Fd:Fe+Fd+Fo].to_numpy(dtype=np.float32))\n",
    "\n",
    "# モデルに入力できる形へ変形\n",
    "enc_in = enc_X.unsqueeze(0).to(device)\n",
    "dec_in = dec_X.unsqueeze(0).to(device)\n",
    "\n",
    "# デコーダに対応する時刻、実測のYを抽出\n",
    "dec_T = df.iloc[s+Te:s+Te+Td, 0:2]\n",
    "obs_Y = df.iloc[s+Te:s+Te+Td, y_cols]\n",
    "\n",
    "# モデルから予測\n",
    "pred_Y_norm = model(enc_in, dec_in)\n",
    "pred_Y_norm = pred_Y_norm.squeeze(0)\n",
    "\n",
    "# 逆標準化\n",
    "ymean = mean.iloc[y_pos]\n",
    "ystd = std.iloc[y_pos]\n",
    "ymean_t = torch.tensor(ymean.values, dtype=torch.float32, device=pred_Y_norm.device).view(1, -1)\n",
    "ystd_t  = torch.tensor(ystd.values,  dtype=torch.float32, device=pred_Y_norm.device).view(1, -1)\n",
    "pred_Y = pred_Y_norm * ystd_t + ymean_t\n",
    "\n",
    "\n",
    "# 1. pandas Series → DataFrame\n",
    "dec_T_df = pd.DataFrame(dec_T).reset_index(drop=True)\n",
    "obs_Y_df  = pd.DataFrame(obs_Y).reset_index(drop=True)\n",
    "\n",
    "# 2. Tensor → NumPy → DataFrame\n",
    "pred_Y_df = pd.DataFrame(pred_Y.detach().cpu().numpy())\n",
    "dec_X_df  = pd.DataFrame(dec_X.detach().cpu().numpy())\n",
    "\n",
    "# 3. 横方向に結合\n",
    "out_df = pd.concat([dec_T_df, obs_Y_df, pred_Y_df, dec_X_df], axis=1)\n",
    "\n",
    "\n",
    "out_df.to_excel(pred_save_path, index=False)\n",
    "\n",
    "print(\"Excelに保存しました:\", pred_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6385c0-9dc0-4b1b-b1c3-5e0085273d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74824b-ef67-45a7-a5d2-50d581ab1bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94826a-ba06-4c98-9ee0-999ab9ef1f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e56f78-e62e-4dce-84ae-2dbecb3b49ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a13189-5184-46d4-a992-8ece87b8e5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ec6f4-81fe-43b2-9831-d51c7e729237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ca8b3-0091-407b-80c2-d81e91f69a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6ca05-0e6f-43c7-8321-920491342e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d1949-b573-4106-baa9-01250cfb9296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfdcfd-9b87-4608-9e44-bb4ce565f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
